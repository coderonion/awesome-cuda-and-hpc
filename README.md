# Awesome-CUDA-and-HPC
[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

ğŸš€ğŸš€ğŸš€ This repository lists some awesome public [CUDA](https://developer.nvidia.com/cuda-zone), [cuda-python](https://github.com/NVIDIA/cuda-python), [cuBLAS](https://developer.nvidia.com/cublas), [CUTLASS](https://github.com/NVIDIA/cutlass), [cuDNN](https://developer.nvidia.com/cudnn), [NCCL](https://github.com/NVIDIA/nccl), [TensorRT](https://developer.nvidia.com/tensorrt), [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM), [Triton](https://github.com/triton-lang/triton), [TileLang](https://github.com/tile-ai/tilelang), [TVM](https://tvm.apache.org/), [MLIR](https://mlir.llvm.org/), [PTX](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html) and High Performance Computing (HPC) projects.

## Contents
- [Awesome-CUDA-and-HPC](#awesome-cuda-and-hpc)
  - [Official Version](#official-version)
  - [Awesome List](#awesome-list)
  - [Learning Resources](#learning-resources)
    - [CUDA Learning](#cuda-learning)
    - [TensorRT Learning](#tensorrt-learning)
    - [Triton Learning](#triton-learning)
    - [TileLang Learning](#tileLang-learning)
    - [TVM Learning](#tvm-learning)
    - [MLIR Learning](#mlir-learning)
    - [HPC Learning](#hpc-learning)
  - [Frameworks](#frameworks)
    - [CUDA Frameworks](#cuda-frameworks)
        - [GPU Interface](#gpu-interface)
            - [CPP Version](#cpp-version)
            - [Python version](#python-version)
            - [Rust Version](#rust-version)
            - [Julia Version](#julia-version)
        - [Performance Benchmark](#performance-benchmark)
        - [Scientific Computing Framework](#scientific-computing-framework)
        - [Attention and Transformer Framework](#attention-and-transformer-framework)
        - [Machine Learning Framework](#machine-learning-framework)
        - [AI Inference Framework](#ai-inference-framework)
            - [LLM Inference and Serving Engine](#llm-inference-and-serving-engine)
            - [High Performance Kernel Library](#high-performance-kernel-library)
            - [C Implementation](#c-implementation)
            - [CPP Implementation](#cpp-implementation)
            - [Mojo Implementation](#mojo-implementation)
            - [Rust Implementation](#rust-implementation)
            - [zig Implementation](#zig-implementation)
            - [Go Implementation](#go-implementation)
        - [Distributed and Multi-GPU Framework](#distributed-and-multi-gpu-framework)
        - [Robotics Framework](#robotics-framework)
        - [ZKP and Web3 Framework](#zkp-and-web3-framework)
    - [Triton Frameworks](#triton-frameworks)
        - [Triton Machine Learning Framework](#triton-machine-learning-framework)
        - [Triton High Performance Kernel Library](#triton-high-performance-kernel-library)
    - [MLIR Frameworks](#mlir-frameworks)
        - [MLIR GPU Programming](#mlir-gpu-programming)
        - [MLIR FFI Bindings](#mlir-ffi-bindings)
        - [MLIR Machine learning Framework](#mlir-machine-learning-framework)
    - [HPC Frameworks](#hpc-frameworks)
  - [Applications](#applications)
    - [CUDA Applications](#cuda-applications)
        - [Image Preprocess](#image-preprocess)
        - [Object Detection](#object-detection)
        - [Signal Processing](#signal-processing)
        - [Mesh Processing](#mesh-processing)
        - [Graph Analytics](#graph-analytics)
  - [Blogs](#blogs)
    - [CUDA and TensorRT Blogs](#cuda-and-tensorrt-blogs)
    - [Triton Blogs](#triton-blogs)
    - [TVM Blogs](#tvm-blogs)
    - [MLIR Blogs](#mlir-blogs)
    - [HPC Blogs](#hpc-blogs)
  - [Videos](#videos)
  - [Interview](#interview)

## Official Version

  - [CUDA](https://developer.nvidia.com/cuda-zone) : CUDA is a parallel computing platform and programming model developed by NVIDIA for general computing on graphical processing units (GPUs).

  - [NVIDIA/cuda-python](https://github.com/NVIDIA/cuda-python) <img src="https://img.shields.io/github/stars/NVIDIA/cuda-python?style=social"/> : CUDA Python: Performance meets Productivity. [nvidia.github.io/cuda-python/](https://nvidia.github.io/cuda-python/)

  - [cuBLAS](https://developer.nvidia.com/cublas) : Basic Linear Algebra on NVIDIA GPUs. NVIDIA cuBLAS is a GPU-accelerated library for accelerating AI and HPC applications. It includes several API extensions for providing drop-in industry standard BLAS APIs and GEMM APIs with support for fusions that are highly optimized for NVIDIA GPUs. The cuBLAS library also contains extensions for batched operations, execution across multiple GPUs, and mixed- and low-precision execution with additional tuning for the best performance.

  - [cuDNN](https://developer.nvidia.com/cudnn) : The NVIDIA CUDA Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. cuDNN provides highly tuned implementations for standard routines such as forward and backward convolution, attention, matmul, pooling, and normalization.

  - [CUTLASS](https://github.com/NVIDIA/cutlass) <img src="https://img.shields.io/github/stars/NVIDIA/cutlass?style=social"/> : CUDA Templates for Linear Algebra Subroutines. CUTLASS is a collection of CUDA C++ template abstractions for implementing high-performance matrix-matrix multiplication (GEMM) and related computations at all levels and scales within CUDA. It incorporates strategies for hierarchical decomposition and data movement similar to those used to implement [cuBLAS](https://developer.nvidia.com/cublas) and [cuDNN](https://developer.nvidia.com/cudnn).

  - [TensorRT](https://github.com/NVIDIA/TensorRT) <img src="https://img.shields.io/github/stars/NVIDIA/TensorRT?style=social"/> : NVIDIAÂ® TensorRTâ„¢ is an SDK for high-performance deep learning inference on NVIDIA GPUs. This repository contains the open source components of TensorRT. [developer.nvidia.com/tensorrt](https://developer.nvidia.com/tensorrt)

  - [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) <img src="https://img.shields.io/github/stars/NVIDIA/TensorRT-LLM?style=social"/> : TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that execute those TensorRT engines. [nvidia.github.io/TensorRT-LLM](https://nvidia.github.io/TensorRT-LLM)

  - [Triton](https://github.com/triton-lang/triton) <img src="https://img.shields.io/github/stars/triton-lang/triton?style=social"/> : Triton is a language and compiler for parallel programming. It aims to provide a Python-based programming environment for productively writing custom DNN compute kernels capable of running at maximal throughput on modern GPU hardware. [triton-lang.org/](https://triton-lang.org/)

  - [TileLang](https://github.com/tile-ai/tilelang) <img src="https://img.shields.io/github/stars/tile-ai/tilelang?style=social"/> : Domain-specific language designed to streamline the development of high-performance GPU/CPU/Accelerators kernels. [tilelang.com/](https://tilelang.com/)

  - [TVM](https://github.com/apache/tvm) <img src="https://img.shields.io/github/stars/apache/tvm?style=social"/> : Open deep learning compiler stack for cpu, gpu and specialized accelerators. [tvm.apache.org/](https://tvm.apache.org/)

  - [TileLang](https://github.com/tile-ai/tilelang) <img src="https://img.shields.io/github/stars/tile-ai/tilelang?style=social"/> : Domain-specific language designed to streamline the development of high-performance GPU/CPU kernels.

  - [MLIR](https://mlir.llvm.org/) : Multi-Level Intermediate Representation Compiler Framework. The MLIR project is a novel approach to building reusable and extensible compiler infrastructure. MLIR aims to address software fragmentation, improve compilation for heterogeneous hardware, significantly reduce the cost of building domain specific compilers, and aid in connecting existing compilers together.

  - [PTX](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html) : PTX, a low-level parallel thread execution virtual machine and instruction set architecture (ISA).



## Awesome List

  - [awesome-cuda-and-hpc](https://github.com/coderonion/awesome-cuda-and-hpc) <img src="https://img.shields.io/github/stars/coderonion/awesome-cuda-and-hpc?style=social"/> : some awesome public [CUDA](https://developer.nvidia.com/cuda-zone), [cuda-python](https://github.com/NVIDIA/cuda-python), [cuBLAS](https://developer.nvidia.com/cublas), [cuDNN](https://developer.nvidia.com/cudnn), [CUTLASS](https://github.com/NVIDIA/cutlass), [TensorRT](https://developer.nvidia.com/tensorrt), [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM), [Triton](https://github.com/triton-lang/triton), [TVM](https://tvm.apache.org/), [MLIR](https://mlir.llvm.org/), [PTX](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html) and High Performance Computing (HPC) projects.

  - [Erkaman/Awesome-CUDA](https://github.com/Erkaman/Awesome-CUDA) <img src="https://img.shields.io/github/stars/Erkaman/Awesome-CUDA?style=social"/> : This is a list of useful libraries and resources for CUDA development.

  - [jslee02/awesome-gpgpu](https://github.com/jslee02/awesome-gpgpu) <img src="https://img.shields.io/github/stars/jslee02/awesome-gpgpu?style=social"/> : ğŸ˜ A curated list of awesome GPGPU (CUDA/OpenCL/Vulkan) resources.

  - [mikeroyal/CUDA-Guide](https://github.com/mikeroyal/CUDA-Guide) <img src="https://img.shields.io/github/stars/mikeroyal/CUDA-Guide?style=social"/> : A guide covering CUDA including the applications and tools that will make you a better and more efficient CUDA developer.

  - [rkinas/triton-resources](https://github.com/rkinas/triton-resources) <img src="https://img.shields.io/github/stars/rkinas/triton-resources?style=social"/> : A curated list of resources for learning and exploring Triton, OpenAI's programming language for writing efficient GPU code.




## Learning Resources


  - [chenzomi12/AISystem](https://github.com/chenzomi12/AISystem) <img src="https://img.shields.io/github/stars/chenzomi12/AISystem?style=social"/> : AISystem ä¸»è¦æ˜¯æŒ‡AIç³»ç»Ÿï¼ŒåŒ…æ‹¬AIèŠ¯ç‰‡ã€AIç¼–è¯‘å™¨ã€AIæ¨ç†å’Œè®­ç»ƒæ¡†æ¶ç­‰AIå…¨æ ˆåº•å±‚æŠ€æœ¯ã€‚

  - [chenzomi12/AIFoundation](https://github.com/chenzomi12/AIFoundation) <img src="https://img.shields.io/github/stars/chenzomi12/AIFoundation?style=social"/> : AIFoundation ä¸»è¦æ˜¯æŒ‡AIç³»ç»Ÿé‡åˆ°å¤§æ¨¡å‹ï¼Œä»åº•å±‚åˆ°ä¸Šå±‚å¦‚ä½•ç³»ç»Ÿçº§åœ°æ”¯æŒå¤§æ¨¡å‹è®­ç»ƒå’Œæ¨ç†ï¼Œå…¨æ ˆçš„æ ¸å¿ƒæŠ€æœ¯ã€‚




  - ### CUDA Learning

    - [NVIDIA CUDA Toolkit Documentation](https://docs.nvidia.com/cuda/) : CUDA Toolkit Documentation.

    - [NVIDIA CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html) : CUDA C++ Programming Guide.

    - [NVIDIA CUDA C++ Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html) : CUDA C++ Best Practices Guide.

    - [NVIDIA cuBLAS Programming Guide](https://docs.nvidia.com/cuda/cublas/index.html) : NVIDIA cuBLAS Documentation.

    - [NVIDIA CUTLASS Programming Guide](https://docs.nvidia.com/cutlass/index.html) : NVIDIA CUTLASS Documentation.

    - [NVIDIA NCCL Programming Guide](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/index.html) : NVIDIA Collective Communication Library (NCCL) Documentation.

    - [NVIDIA PTX(Parallel Thread Execution) Programming Guide](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html) : NVIDIA PTX (Parallel Thread Execution) Programming Guide.

    - [NVIDIA/cuda-samples](https://github.com/NVIDIA/cuda-samples) <img src="https://img.shields.io/github/stars/NVIDIA/cuda-samples?style=social"/> : Samples for CUDA Developers which demonstrates features in CUDA Toolkit.

    - [NVIDIA/CUDALibrarySamples](https://github.com/NVIDIA/CUDALibrarySamples) <img src="https://img.shields.io/github/stars/NVIDIA/CUDALibrarySamples?style=social"/> : CUDA Library Samples.

    - [NVIDIA/cuda-python](https://github.com/NVIDIA/cuda-python) <img src="https://img.shields.io/github/stars/NVIDIA/cuda-python?style=social"/> : CUDA Python: Performance meets Productivity. [nvidia.github.io/cuda-python/](https://nvidia.github.io/cuda-python/)

    - [CuPy](https://github.com/cupy/cupy) <img src="https://img.shields.io/github/stars/cupy/cupy?style=social"/> : CuPy : NumPy & SciPy for GPU. [cupy.dev](https://cupy.dev/). [CuPy User Guide](https://docs.cupy.dev/en/stable/user_guide/)

    - [NVIDIA-developer-blog/code-samples](https://github.com/NVIDIA-developer-blog/code-samples) <img src="https://img.shields.io/github/stars/NVIDIA-developer-blog/code-samples?style=social"/> : Source code examples from the [Parallel Forall Blog](http://developer.nvidia.com/parallel-forall).

    - [HeKun-NVIDIA/CUDA-Programming-Guide-in-Chinese](https://github.com/HeKun-NVIDIA/CUDA-Programming-Guide-in-Chinese) <img src="https://img.shields.io/github/stars/HeKun-NVIDIA/CUDA-Programming-Guide-in-Chinese?style=social"/> : This is a Chinese translation of the CUDA programming guide. æœ¬é¡¹ç›®ä¸º CUDA C Programming Guide çš„ä¸­æ–‡ç¿»è¯‘ç‰ˆã€‚

    - [brucefan1983/CUDA-Programming](https://github.com/brucefan1983/CUDA-Programming) <img src="https://img.shields.io/github/stars/brucefan1983/CUDA-Programming?style=social"/> : Sample codes for my CUDA programming book.

    - [YouQixiaowu/CUDA-Programming-with-Python](https://github.com/YouQixiaowu/CUDA-Programming-with-Python) <img src="https://img.shields.io/github/stars/YouQixiaowu/CUDA-Programming-with-Python?style=social"/> :  å…³äºä¹¦ç±CUDA Programmingä½¿ç”¨äº†pycudaæ¨¡å—çš„Pythonç‰ˆæœ¬çš„ç¤ºä¾‹ä»£ç ã€‚

    - [QINZHAOYU/CudaSteps](https://github.com/QINZHAOYU/CudaSteps) <img src="https://img.shields.io/github/stars/QINZHAOYU/CudaSteps?style=social"/> : åŸºäºã€Šcudaç¼–ç¨‹-åŸºç¡€ä¸å®è·µã€‹ï¼ˆæ¨Šå“²å‹‡ è‘—ï¼‰çš„cudaå­¦ä¹ ä¹‹è·¯ã€‚

    - [MAhaitao999/CUDA_Programming](https://github.com/MAhaitao999/CUDA_Programming) <img src="https://img.shields.io/github/stars/MAhaitao999/CUDA_Programming?style=social"/> : ã€ŠCUDAç¼–ç¨‹åŸºç¡€ä¸å®è·µã€‹ä¸€ä¹¦çš„ä»£ç ã€‚

    - [xlite-dev/LeetCUDA](https://github.com/xlite-dev/LeetCUDA) <img src="https://img.shields.io/github/stars/xlite-dev/LeetCUDA?style=social"/> : ğŸ“šLeetCUDA: Modern CUDA Learn Notes with PyTorch for BeginnersğŸ‘, 200+ CUDA/Tensor Cores Kernels, HGEMM, FA-2 MMA etc.ğŸ”¥ [https://github.com/xlite-dev/LeetCUDA](https://github.com/xlite-dev/LeetCUDA)

    - [BBuf/how-to-optim-algorithm-in-cuda](https://github.com/BBuf/how-to-optim-algorithm-in-cuda) <img src="https://img.shields.io/github/stars/BBuf/how-to-optim-algorithm-in-cuda?style=social"/> : how to optimize some algorithm in cuda.

    - [PaddleJitLab/CUDATutorial](https://github.com/PaddleJitLab/CUDATutorial) <img src="https://img.shields.io/github/stars/PaddleJitLab/CUDATutorial?style=social"/> : A self-learning tutorail for CUDA High Performance Programing. ä»é›¶å¼€å§‹å­¦ä¹  CUDA é«˜æ€§èƒ½ç¼–ç¨‹ã€‚

    - [RussWong/CUDATutorial](https://github.com/RussWong/CUDATutorial) <img src="https://img.shields.io/github/stars/RussWong/CUDATutorial?style=social"/> : A CUDA tutorial to make people learn CUDA program from 0.

    - [RichardAns/CUDA-Programs](https://github.com/RichardAns/CUDA-Programs) <img src="https://img.shields.io/github/stars/RichardAns/CUDA-Programs?style=social"/> : Examples from Programming in Parallel with CUDA.

    - [bertmaher/simplegemm](https://github.com/bertmaher/simplegemm) <img src="https://img.shields.io/github/stars/bertmaher/simplegemm?style=social"/> : Pingpong GEMM from scratch. I've been really excited to learn the lowest-level details of GPU matrix multiplication recently, so I was really inspired to read Pranjal Shankhdhar's fantastic blog post [Outperforming cuBLAS on H100](https://cudaforfun.substack.com/p/outperforming-cublas-on-h100-a-worklog), which implements a fast gemm from first principles in CUDA, and actually outperforms cuBLAS. In a similar vein, I wanted to understand the [pingpong](https://github.com/NVIDIA/cutlass/blob/main/media/docs/efficient_gemm.md#hopper-warp-specialization) gemm algorithm in detail. So, I used [https://github.com/pranjalssh/fast.cu](https://github.com/pranjalssh/fast.cu) as a starting point, and wrote this kernel to see if I could match CUTLASS's pingpong implementation myself, using hand-written CUDA.

    - [pranjalssh/fast.cu](https://github.com/pranjalssh/fast.cu) <img src="https://img.shields.io/github/stars/pranjalssh/fast.cu?style=social"/> : Fastest GPU kernels, written from scratch. Matrix multiplication of square bf16 matrices, accumulated in fp32. Explanation in [https://cudaforfun.substack.com/p/outperforming-cublas-on-h100-a-worklog](https://cudaforfun.substack.com/p/outperforming-cublas-on-h100-a-worklog)

    - [gpu-mode/lectures](https://github.com/gpu-mode/lectures) <img src="https://img.shields.io/github/stars/gpu-mode/lectures?style=social"/> : Material for gpu-mode lectures. [www.youtube.com/@GPUMODE](https://www.youtube.com/@GPUMODE)

    - [gpu-mode/resource-stream](https://github.com/gpu-mode/resource-stream) <img src="https://img.shields.io/github/stars/cuda-mode/resource-stream?style=social"/> :GPU programming related news and material links. [discord.gg/gpumode](https://discord.gg/gpumode)

    - [ifromeast/cuda_learning](https://github.com/ifromeast/cuda_learning) <img src="https://img.shields.io/github/stars/ifromeast/cuda_learning?style=social"/> : learning how CUDA works.

    - [a-hamdi/cuda](https://github.com/a-hamdi/cuda) <img src="https://img.shields.io/github/stars/a-hamdi/cuda?style=social"/> : 100 days of building Cuda kernels! This document serves as a log of the progress and knowledge I gained while working on CUDA programming and studying the PMPP (Parallel Programming and Optimization) book. Mentor: [https://github.com/hkproj/](https://github.com/hkproj/). Bro in the 100 days challenge: [https://github.com/1y33/100Days](https://github.com/1y33/100Days).

    - [SwekeR-463/100kernels](https://github.com/SwekeR-463/100kernels) <img src="https://img.shields.io/github/stars/SwekeR-463/100kernels?style=social"/> : 100 days of learning & making kernels in cuda / triton.

    - [Tongkaio/CUDA_Kernel_Samples](https://github.com/Tongkaio/CUDA_Kernel_Samples) <img src="https://img.shields.io/github/stars/Tongkaio/CUDA_Kernel_Samples?style=social"/> : CUDA ç®—å­æ‰‹æ’•ä¸é¢è¯•æŒ‡å—ã€‚

    - [leimao/CUDA-GEMM-Optimization](https://github.com/leimao/CUDA-GEMM-Optimization) <img src="https://img.shields.io/github/stars/leimao/CUDA-GEMM-Optimization?style=social"/> : [CUDA Matrix Multiplication Optimization](https://leimao.github.io/article/CUDA-Matrix-Multiplication-Optimization/). This repository contains the CUDA kernels for general matrix-matrix multiplication (GEMM) and the corresponding performance analysis.

    - [interestingLSY/CUDA-From-Correctness-To-Performance-Code](https://github.com/interestingLSY/CUDA-From-Correctness-To-Performance-Code) <img src="https://img.shields.io/github/stars/interestingLSY/CUDA-From-Correctness-To-Performance-Code?style=social"/> : Codes & examples for "CUDA - From Correctness to Performance". The lecture can be found at [https://wiki.lcpu.dev/zh/hpc/from-scratch/cuda](https://wiki.lcpu.dev/zh/hpc/from-scratch/cuda).

    - [Liu-xiandong/How_to_optimize_in_GPU](https://github.com/Liu-xiandong/How_to_optimize_in_GPU) <img src="https://img.shields.io/github/stars/Liu-xiandong/How_to_optimize_in_GPU?style=social"/> : This is a series of GPU optimization topics. Here we will introduce how to optimize the CUDA kernel in detail. I will introduce several basic kernel optimizations, including: elementwise, reduce, sgemv, sgemm, etc. The performance of these kernels is basically at or near the theoretical limit.

    - [tpoisonooo/how-to-optimize-gemm](https://github.com/tpoisonooo/how-to-optimize-gemm) <img src="https://img.shields.io/github/stars/tpoisonooo/how-to-optimize-gemm?style=social"/> : row-major matmul optimization. [zhuanlan.zhihu.com/p/65436463](https://zhuanlan.zhihu.com/p/65436463).

    - [Bruce-Lee-LY/matrix_multiply](https://github.com/Bruce-Lee-LY/matrix_multiply) <img src="https://img.shields.io/github/stars/Bruce-Lee-LY/matrix_multiply?style=social"/> : Several common methods of matrix multiplication are implemented on CPU and Nvidia GPU using C++11 and CUDA.

    - [Bruce-Lee-LY/cuda_hgemm](https://github.com/Bruce-Lee-LY/cuda_hgemm) <img src="https://img.shields.io/github/stars/Bruce-Lee-LY/cuda_hgemm?style=social"/> : Several optimization methods of half-precision general matrix multiplication (HGEMM) using tensor core with WMMA API and MMA PTX instruction.

    - [Bruce-Lee-LY/cuda_hgemv](https://github.com/Bruce-Lee-LY/cuda_hgemv) <img src="https://img.shields.io/github/stars/Bruce-Lee-LY/cuda_hgemv?style=social"/> : Several optimization methods of half-precision general matrix vector multiplication (HGEMV) using CUDA core.

    - [enp1s0/ozIMMU](https://github.com/enp1s0/ozIMMU) <img src="https://img.shields.io/github/stars/enp1s0/ozIMMU?style=social"/> : FP64 equivalent GEMM via Int8 Tensor Cores using the Ozaki scheme. [arxiv.org/abs/2306.11975](https://arxiv.org/abs/2306.11975)

    - [Cjkkkk/CUDA_gemm](https://github.com/Cjkkkk/CUDA_gemm) <img src="https://img.shields.io/github/stars/Cjkkkk/CUDA_gemm?style=social"/> : A simple high performance CUDA GEMM implementation.

    - [AyakaGEMM/Hands-on-GEMM](https://github.com/AyakaGEMM/Hands-on-GEMM) <img src="https://img.shields.io/github/stars/AyakaGEMM/Hands-on-GEMM?style=social"/> : A GEMM tutorial.

    - [zpzim/MSplitGEMM](https://github.com/zpzim/MSplitGEMM) <img src="https://img.shields.io/github/stars/zpzim/MSplitGEMM?style=social"/> : Large matrix multiplication in CUDA.

    - [jundaf2/CUDA-INT8-GEMM](https://github.com/jundaf2/CUDA-INT8-GEMM) <img src="https://img.shields.io/github/stars/jundaf2/CUDA-INT8-GEMM?style=social"/> : CUDA 8-bit Tensor Core Matrix Multiplication based on m16n16k16 WMMA API.

    - [chanzhennan/cuda_gemm_benchmark](https://github.com/chanzhennan/cuda_gemm_benchmark) <img src="https://img.shields.io/github/stars/chanzhennan/cuda_gemm_benchmark?style=social"/> : Base on gtest/benchmark, refer to [https://github.com/Liu-xiandong/How_to_optimize_in_GPU](https://github.com/Liu-xiandong/How_to_optimize_in_GPU).

    - [YuxueYang1204/CudaDemo](https://github.com/YuxueYang1204/CudaDemo) <img src="https://img.shields.io/github/stars/YuxueYang1204/CudaDemo?style=social"/> : Implement custom operators in PyTorch with cuda/c++.

    - [CoffeeBeforeArch/cuda_programming](https://github.com/CoffeeBeforeArch/cuda_programming) <img src="https://img.shields.io/github/stars/CoffeeBeforeArch/cuda_programming?style=social"/> : Code from the "CUDA Crash Course" YouTube series by CoffeeBeforeArch.

    - [rbaygildin/learn-gpgpu](https://github.com/rbaygildin/learn-gpgpu) <img src="https://img.shields.io/github/stars/rbaygildin/learn-gpgpu?style=social"/> : Algorithms implemented in CUDA + resources about GPGPU.

    - [godweiyang/NN-CUDA-Example](https://github.com/godweiyang/NN-CUDA-Example) <img src="https://img.shields.io/github/stars/godweiyang/NN-CUDA-Example?style=social"/> : Several simple examples for popular neural network toolkits calling custom CUDA operators.

    - [yhwang-hub/Matrix_Multiplication_Performance_Optimization](https://github.com/yhwang-hub/Matrix_Multiplication_Performance_Optimization) <img src="https://img.shields.io/github/stars/yhwang-hub/Matrix_Multiplication_Performance_Optimization?style=social"/> : Matrix Multiplication Performance Optimization.

    - [caiwanxianhust/ClusteringByCUDA](https://github.com/caiwanxianhust/ClusteringByCUDA) <img src="https://img.shields.io/github/stars/caiwanxianhust/ClusteringByCUDA?style=social"/> : ä½¿ç”¨ CUDA C++ å®ç°çš„ä¸€ç³»åˆ—èšç±»ç®—æ³•ã€‚

    - [ulrichstern/cuda-convnet](https://github.com/ulrichstern/cuda-convnet) <img src="https://img.shields.io/github/stars/ulrichstern/cuda-convnet?style=social"/> : Alex Krizhevsky's original code from Google Code. "å¾®ä¿¡å…¬ä¼—å·ã€Œäººå·¥æ™ºèƒ½å¤§è®²å ‚ã€ã€Š[æ‰¾åˆ°äº†AlexNetå½“å¹´çš„æºä»£ç ï¼Œæ²¡ç”¨æ¡†æ¶ï¼Œä»é›¶æ‰‹æ’¸CUDA/C++](https://mp.weixin.qq.com/s/plxXG8y5QlxSionyjyPXqw)ã€‹"ã€‚

    - [PacktPublishing/Learn-CUDA-Programming](https://github.com/PacktPublishing/Learn-CUDA-Programming) <img src="https://img.shields.io/github/stars/PacktPublishing/Learn-CUDA-Programming?style=social"/> : Learn CUDA Programming, published by Packt.

    - [PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA) <img src="https://img.shields.io/github/stars/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA?style=social"/> : Hands-On GPU Programming with Python and CUDA, published by Packt.

    - [PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA) <img src="https://img.shields.io/github/stars/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA?style=social"/> : Hands-On GPU Accelerated Computer Vision with OpenCV and CUDA, published by Packt.

    - [BobMcDear/neural-network-cuda](https://github.com/BobMcDear/neural-network-cuda) <img src="https://img.shields.io/github/stars/BobMcDear/neural-network-cuda?style=social"/> : Neural network from scratch in CUDA/C++.

    - [zjhellofss/KuiperLLama](https://github.com/zjhellofss/KuiperLLama) <img src="https://img.shields.io/github/stars/zjhellofss/KuiperLLama?style=social"/> : ã€ŠåŠ¨æ‰‹è‡ªåˆ¶å¤§æ¨¡å‹æ¨ç†æ¡†æ¶ã€‹ã€‚KuiperLLama åŠ¨æ‰‹è‡ªåˆ¶å¤§æ¨¡å‹æ¨ç†æ¡†æ¶ï¼Œæ”¯æŒLLama2/3å’ŒQwen2.5ã€‚æ ¡æ‹›ã€ç§‹æ‹›ã€æ˜¥æ‹›ã€å®ä¹ å¥½é¡¹ç›®ï¼Œå¸¦ä½ ä»é›¶åŠ¨æ‰‹å®ç°æ”¯æŒLLama2/3å’ŒQwen2.5çš„å¤§æ¨¡å‹æ¨ç†æ¡†æ¶ã€‚

    - [zjhellofss/KuiperInfer](https://github.com/zjhellofss/KuiperInfer) <img src="https://img.shields.io/github/stars/zjhellofss/KuiperInfer?style=social"/> :  æ ¡æ‹›ã€ç§‹æ‹›ã€æ˜¥æ‹›ã€å®ä¹ å¥½é¡¹ç›®ï¼å¸¦ä½ ä»é›¶å®ç°ä¸€ä¸ªé«˜æ€§èƒ½çš„æ·±åº¦å­¦ä¹ æ¨ç†åº“ï¼Œæ”¯æŒå¤§æ¨¡å‹ llama2 ã€Unetã€Yolov5ã€Resnetç­‰æ¨¡å‹çš„æ¨ç†ã€‚Implement a high-performance deep learning inference library step by stepã€‚

    - [zjhellofss/kuiperdatawhale](https://github.com/zjhellofss/kuiperdatawhale) <img src="https://img.shields.io/github/stars/zjhellofss/kuiperdatawhale?style=social"/> :  ä»é›¶è‡ªåˆ¶æ·±åº¦å­¦ä¹ æ¨ç†æ¡†æ¶ã€‚

    - [MarioSieg/magnetron](https://github.com/MarioSieg/magnetron) <img src="https://img.shields.io/github/stars/MarioSieg/magnetron?style=social"/> :  (WIP) A small but powerful, homemade PyTorch from scratch. Minimalistic homemade PyTorch alternative, written in C99 and Python.

    - [lucasdelimanogueira/PyNorch](https://github.com/lucasdelimanogueira/PyNorch) <img src="https://img.shields.io/github/stars/lucasdelimanogueira/PyNorch?style=social"/> :  Recreating PyTorch from scratch (C/C++, CUDA, NCCL and Python, with multi-GPU support and automatic differentiation!)

    - [xgqdut2016/cuda_code](https://github.com/xgqdut2016/cuda_code) <img src="https://img.shields.io/github/stars/xgqdut2016/cuda_code?style=social"/> : easy cuda code. CUDAä»£ç ç®€å•å…¥é—¨ã€‚

    - [xgqdut2016/hpc_project](https://github.com/xgqdut2016/hpc_project) <img src="https://img.shields.io/github/stars/xgqdut2016/hpc_project?style=social"/> : some hpc project for learning.

    - [xgqdut2016/hpc2torch](https://github.com/xgqdut2016/hpc2torch) <img src="https://img.shields.io/github/stars/xgqdut2016/hpc2torch?style=social"/> : è¿™ä¸ªä»“åº“æ‰“ç®—æ­å»ºä¸€ä¸ªé«˜æ€§èƒ½åº•å±‚åº“çš„æµ‹è¯•æ¡†æ¶ï¼Œå°†ä¼šé’ˆå¯¹onnxçš„ç®—å­ç¼–å†™ç›¸å…³çš„é«˜æ€§èƒ½kernelï¼Œä½œä¸ºpytorchçš„è¡¥å……ï¼Œä»pythonç«¯å¯¹æ¯”æ‰‹å†™kernelå’Œpytorchåº“å‡½æ•°çš„æ€§èƒ½ä»¥åŠç²¾åº¦å¯¹æ¯”ã€‚

    - [zoheth/yan](https://github.com/zoheth/yan) <img src="https://img.shields.io/github/stars/zoheth/yan?style=social"/> : Yan (ç‚) is a high-performance CUDA operator library designed for learning purposes while emphasizing clean code and maximum performance.




  - ### TensorRT Learning

    - [NVIDIA TensorRT Docs](https://docs.nvidia.com/deeplearning/tensorrt/) : NVIDIA Deep Learning TensorRT Documentation.

    - [TensorRT](https://github.com/NVIDIA/TensorRT) <img src="https://img.shields.io/github/stars/NVIDIA/TensorRT?style=social"/> : NVIDIAÂ® TensorRTâ„¢ is an SDK for high-performance deep learning inference on NVIDIA GPUs. This repository contains the open source components of TensorRT. [developer.nvidia.com/tensorrt](https://developer.nvidia.com/tensorrt)

    - [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) <img src="https://img.shields.io/github/stars/NVIDIA/TensorRT-LLM?style=social"/> : TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that execute those TensorRT engines. [nvidia.github.io/TensorRT-LLM](https://nvidia.github.io/TensorRT-LLM)

    - [HeKun-NVIDIA/TensorRT-Developer_Guide_in_Chinese](https://github.com/HeKun-NVIDIA/TensorRT-Developer_Guide_in_Chinese) <img src="https://img.shields.io/github/stars/HeKun-NVIDIA/TensorRT-Developer_Guide_in_Chinese?style=social"/> : æœ¬é¡¹ç›®æ˜¯NVIDIA TensorRTçš„ä¸­æ–‡ç‰ˆå¼€å‘æ‰‹å†Œï¼Œ æœ‰ä¸ªäººç¿»è¯‘å¹¶æ·»åŠ è‡ªå·±çš„ç†è§£ã€‚

    - [kalfazed/tensorrt_starter](https://github.com/kalfazed/tensorrt_starter) <img src="https://img.shields.io/github/stars/kalfazed/tensorrt_starter?style=social"/> : This repository give a guidline to learn CUDA and TensorRT from the beginning.

    - [LitLeo/TensorRT_Tutorial](https://github.com/LitLeo/TensorRT_Tutorial) <img src="https://img.shields.io/github/stars/LitLeo/TensorRT_Tutorial?style=social"/> : TensorRT_Tutorial.





  - ### Triton Learning

    - [Triton](https://github.com/triton-lang/triton) <img src="https://img.shields.io/github/stars/triton-lang/triton?style=social"/> : Development repository for the Triton language and compiler. [triton-lang.org/](https://triton-lang.org/)

    - [Triton Docs](https://triton-lang.org/main/index.html) : Triton Documentation.

    - [hyperai/triton-cn](https://github.com/hyperai/triton-cn) <img src="https://img.shields.io/github/stars/hyperai/triton-cn?style=social"/> : Triton Documentation in Chinese Simplified / Triton ä¸­æ–‡æ–‡æ¡£. [triton.hyper.ai](https://triton.hyper.ai/)



  - ### TileLang Learning

    - [TileLang](https://github.com/tile-ai/tilelang) <img src="https://img.shields.io/github/stars/tile-ai/tilelang?style=social"/> : Domain-specific language designed to streamline the development of high-performance GPU/CPU/Accelerators kernels. [tilelang.com/](https://tilelang.com/)

    - [TileLang Docs](https://tilelang.com/) : TileLang Documentation.




  - ### TVM Learning

    - [Apache TVM ä¸­æ–‡ç«™](https://tvm.hyper.ai/) : Apache TVM ä¸­æ–‡æ–‡æ¡£ï¼



  - ### MLIR Learning

    - [LLVM Docs](https://llvm.org/docs/) : LLVM Documentation.

    - [MLIR Docs](https://mlir.llvm.org/docs/) : MLIR Code Documentation.

    - [BBuf/tvm_mlir_learn](https://github.com/BBuf/tvm_mlir_learn) <img src="https://img.shields.io/github/stars/BBuf/tvm_mlir_learn?style=social"/> : compiler learning resources collect.

    - [j2kun/mlir-tutorial](https://github.com/j2kun/mlir-tutorial) <img src="https://img.shields.io/github/stars/j2kun/mlir-tutorial?style=social"/> : This is the code repository for a series of articles on the [MLIR framework](https://mlir.llvm.org/) for building compilers.

    - [KEKE046/mlir-tutorial](https://github.com/KEKE046/mlir-tutorial) <img src="https://img.shields.io/github/stars/KEKE046/mlir-tutorial?style=social"/> : Hands-On Practical MLIR Tutorial.

    - [AyakaGEMM/Hands-on-MLIR](https://github.com/AyakaGEMM/Hands-on-MLIR) <img src="https://img.shields.io/github/stars/AyakaGEMM/Hands-on-MLIR?style=social"/> : Hands-on-MLIR.

    - [yao-jiashu/KernelCodeGen](https://github.com/yao-jiashu/KernelCodeGen) <img src="https://img.shields.io/github/stars/yao-jiashu/KernelCodeGen?style=social"/> : GEMM/Conv2d CUDA/HIP kernel code generation using MLIR.



  - ### HPC Learning

    - [LAFF-On-PfHP](https://www.cs.utexas.edu/~flame/laff/pfhp/LAFF-On-PfHP.html) : LAFF-On Programming for High Performance.

    - [flame/how-to-optimize-gemm](https://github.com/flame/how-to-optimize-gemm) <img src="https://img.shields.io/github/stars/flame/how-to-optimize-gemm?style=social"/> : How To Optimize Gemm wiki pages. [https://github.com/flame/how-to-optimize-gemm/wiki](https://github.com/flame/how-to-optimize-gemm/wiki)

    - [flame/blislab](https://github.com/flame/blislab) <img src="https://img.shields.io/github/stars/flame/blislab?style=social"/> : BLISlab: A Sandbox for Optimizing GEMM. Check the [tutorial](https://github.com/flame/blislab/blob/master/tutorial.pdf) for more details.

    - [tpoisonooo/how-to-optimize-gemm](https://github.com/tpoisonooo/how-to-optimize-gemm) <img src="https://img.shields.io/github/stars/tpoisonooo/how-to-optimize-gemm?style=social"/> : row-major matmul optimization. [zhuanlan.zhihu.com/p/65436463](https://zhuanlan.zhihu.com/p/65436463).

    - [YichengDWu/matmul.mojo](https://github.com/YichengDWu/matmul.mojo) <img src="https://img.shields.io/github/stars/YichengDWu/matmul.mojo?style=social"/> : High Performance Matrix Multiplication in Pure Mojo ğŸ”¥





## Frameworks

  - ### CUDA Frameworks

    - #### GPU Interface
      ##### GPUæ¥å£

        - ##### CPP Version

            - [CCCL](https://github.com/NVIDIA/cccl) <img src="https://img.shields.io/github/stars/NVIDIA/cccl?style=social"/> : CUDA C++ Core Libraries. The concept for the CUDA C++ Core Libraries (CCCL) grew organically out of the Thrust, CUB, and libcudacxx projects that were developed independently over the years with a similar goal: to provide high-quality, high-performance, and easy-to-use C++ abstractions for CUDA developers.

            - [HIP](https://github.com/ROCm/HIP) <img src="https://img.shields.io/github/stars/ROCm/HIP?style=social"/> : HIP: C++ Heterogeneous-Compute Interface for Portability. HIP is a C++ Runtime API and Kernel Language that allows developers to create portable applications for AMD and NVIDIA GPUs from single source code. [rocmdocs.amd.com/projects/HIP/](https://rocmdocs.amd.com/projects/HIP/)


        - ##### Python Version

            - [NVIDIA/cuda-python](https://github.com/NVIDIA/cuda-python) <img src="https://img.shields.io/github/stars/NVIDIA/cuda-python?style=social"/> : CUDA Python is the home for accessing NVIDIAâ€™s CUDA platform from Python. CUDA Python Low-level Bindings. [nvidia.github.io/cuda-python/](https://nvidia.github.io/cuda-python/latest/)

            - [CuPy](https://github.com/cupy/cupy) <img src="https://img.shields.io/github/stars/cupy/cupy?style=social"/> : CuPy : NumPy & SciPy for GPU. [cupy.dev](https://cupy.dev/)

            - [PyCUDA](https://github.com/inducer/pycuda) <img src="https://img.shields.io/github/stars/inducer/pycuda?style=social"/> : PyCUDA: Pythonic Access to CUDA, with Arrays and Algorithms. [mathema.tician.de/software/pycuda](http://mathema.tician.de/software/pycuda)



        - ##### Rust Version

            - [jessfraz/advent-of-cuda](https://github.com/jessfraz/advent-of-cuda) <img src="https://img.shields.io/github/stars/jessfraz/advent-of-cuda?style=social"/> : Doing advent of code with CUDA and rust.

            - [Bend](https://github.com/HigherOrderCO/Bend) <img src="https://img.shields.io/github/stars/HigherOrderCO/Bend?style=social"/> : A massively parallel, high-level programming language.[higherorderco.com](https://higherorderco.com/)

            - [HVM](https://github.com/HigherOrderCO/HVM) <img src="https://img.shields.io/github/stars/HigherOrderCO/HVM?style=social"/> : A massively parallel, optimal functional runtime in Rust.[higherorderco.com](https://higherorderco.com/)

            - [ZLUDA](https://github.com/vosen/ZLUDA) <img src="https://img.shields.io/github/stars/vosen/ZLUDA?style=social"/> : CUDA on AMD GPUs.

            - [Rust-CUDA](https://github.com/Rust-GPU/Rust-CUDA) <img src="https://img.shields.io/github/stars/Rust-GPU/Rust-CUDA?style=social"/> : Ecosystem of libraries and tools for writing and executing fast GPU code fully in Rust.

            - [cudarc](https://github.com/coreylowman/cudarc) <img src="https://img.shields.io/github/stars/coreylowman/cudarc?style=social"/> : cudarc: minimal and safe api over the cuda toolkit.

            - [bindgen_cuda](https://github.com/Narsil/bindgen_cuda) <img src="https://img.shields.io/github/stars/Narsil/bindgen_cuda?style=social"/> : Similar crate than [bindgen](https://github.com/rust-lang/rust-bindgen) in philosophy. It will help create automatic bindgen to cuda kernels source files and make them easier to use directly from Rust.

            - [cuda-driver](https://github.com/YdrMaster/cuda-driver) <img src="https://img.shields.io/github/stars/YdrMaster/cuda-driver?style=social"/> : åŸºäº CUDA Driver API çš„ cuda è¿è¡Œæ—¶ç¯å¢ƒã€‚

            - [async-cuda](https://github.com/oddity-ai/async-cuda) <img src="https://img.shields.io/github/stars/oddity-ai/async-cuda?style=social"/> : Asynchronous CUDA for Rust.

            - [async-tensorrt](https://github.com/oddity-ai/async-tensorrt) <img src="https://img.shields.io/github/stars/oddity-ai/async-tensorrt?style=social"/> : Asynchronous TensorRT for Rust.

            - [krnl](https://github.com/charles-r-earp/krnl) <img src="https://img.shields.io/github/stars/charles-r-earp/krnl?style=social"/> : Safe, portable, high performance compute (GPGPU) kernels.

            - [custos](https://github.com/elftausend/custos) <img src="https://img.shields.io/github/stars/elftausend/custos?style=social"/> : A minimal OpenCL, CUDA, WGPU and host CPU array manipulation engine / framework.

            - [spinorml/nvlib](https://github.com/spinorml/nvlib) <img src="https://img.shields.io/github/stars/spinorml/nvlib?style=social"/> : Rust interoperability with NVIDIA CUDA NVRTC and Driver.

            - [DoeringChristian/cuda-rs](https://github.com/DoeringChristian/cuda-rs) <img src="https://img.shields.io/github/stars/DoeringChristian/cuda-rs?style=social"/> : Cuda Bindings for rust generated with bindgen-cli (similar to cust_raw).

            - [romankoblov/rust-nvrtc](https://github.com/romankoblov/rust-nvrtc) <img src="https://img.shields.io/github/stars/romankoblov/rust-nvrtc?style=social"/> : NVRTC bindings for RUST.

            - [solkitten/astro-cuda](https://github.com/solkitten/astro-cuda) <img src="https://img.shields.io/github/stars/solkitten/astro-cuda?style=social"/> : CUDA Driver API bindings for Rust.

            - [bokutotu/curs](https://github.com/bokutotu/curs) <img src="https://img.shields.io/github/stars/bokutotu/curs?style=social"/> : cuda&cublas&cudnn wrapper for Rust.

            - [rust-cuda/cuda-sys](https://github.com/rust-cuda/cuda-sys) <img src="https://img.shields.io/github/stars/rust-cuda/cuda-sys?style=social"/> : Rust binding to CUDA APIs.

            - [bheisler/RustaCUDA](https://github.com/bheisler/RustaCUDA) <img src="https://img.shields.io/github/stars/bheisler/RustaCUDA?style=social"/> : Rusty wrapper for the CUDA Driver API.

            - [tmrob2/cuda2rust_sandpit](https://github.com/tmrob2/cuda2rust_sandpit) <img src="https://img.shields.io/github/stars/tmrob2/cuda2rust_sandpit?style=social"/> : Minimal examples to get CUDA linear algebra programs working with Rust using CC & FFI.

            - [PhDP/rust-cuda-template](https://github.com/PhDP/rust-cuda-template) <img src="https://img.shields.io/github/stars/PhDP/rust-cuda-template?style=social"/> : Simple template for Rust + CUDA.

            - [neka-nat/cuimage](https://github.com/neka-nat/cuimage) <img src="https://img.shields.io/github/stars/neka-nat/cuimage?style=social"/> : Rust implementation of image processing library with CUDA.

            - [yanghaku/cuda-driver-sys](https://github.com/yanghaku/cuda-driver-sys) <img src="https://img.shields.io/github/stars/yanghaku/cuda-driver-sys?style=social"/> : Rust binding to CUDA Driver APIs.

            - [Canyon-ml/canyon-sys](https://github.com/Canyon-ml/canyon-sys) <img src="https://img.shields.io/github/stars/Canyon-ml/canyon-sys?style=social"/> : Rust Bindings for Cuda, CuDNN.

            - [cea-hpc/HARP](https://github.com/cea-hpc/HARP) <img src="https://img.shields.io/github/stars/cea-hpc/HARP?style=social"/> : Small tool for profiling the performance of hardware-accelerated Rust code using OpenCL and CUDA.

            - [Conqueror712/CUDA-Simulator](https://github.com/Conqueror712/CUDA-Simulator) <img src="https://img.shields.io/github/stars/Conqueror712/CUDA-Simulator?style=social"/> : A self-developed version of the user-mode CUDA emulator project and a learning repository for Rust.

            - [cszach/rust-cuda-template](https://github.com/cszach/rust-cuda-template) <img src="https://img.shields.io/github/stars/cszach/rust-cuda-template?style=social"/> : A Rust CUDA template with detailed instructions.

            - [exor2008/fluid-simulator](https://github.com/exor2008/fluid-simulator) <img src="https://img.shields.io/github/stars/exor2008/fluid-simulator?style=social"/> : Rust CUDA fluid simulator.

            - [chichieinstein/rustycuda](https://github.com/chichieinstein/rustycuda) <img src="https://img.shields.io/github/stars/chichieinstein/rustycuda?style=social"/> : Convenience functions for generic handling of CUDA resources on the Rust side.

            - [Jafagervik/cruda](https://github.com/Jafagervik/cruda) <img src="https://img.shields.io/github/stars/Jafagervik/cruda?style=social"/> : CRUDA - Writing rust with cuda.

            - [lennyerik/cutransform](https://github.com/lennyerik/cutransform) <img src="https://img.shields.io/github/stars/lennyerik/cutransform?style=social"/> : CUDA kernels in any language supported by LLVM.

           - [cjordan/hip-sys](https://github.com/cjordan/hip-sys) <img src="https://img.shields.io/github/stars/cjordan/hip-sys?style=social"/> : Rust bindings for HIP.

            - [rust-gpu](https://github.com/EmbarkStudios/rust-gpu) <img src="https://img.shields.io/github/stars/EmbarkStudios/rust-gpu?style=social"/> : ğŸ‰ Making Rust a first-class language and ecosystem for GPU shaders ğŸš§ [shader.rs](https://shader.rs/)

            - [wgpu](https://github.com/gfx-rs/wgpu) <img src="https://img.shields.io/github/stars/gfx-rs/wgpu?style=social"/> : Safe and portable GPU abstraction in Rust, implementing WebGPU API. [wgpu.rs](https://wgpu.rs/)

            - [Vulkano](https://github.com/vulkano-rs/vulkano) <img src="https://img.shields.io/github/stars/vulkano-rs/vulkano?style=social"/> : Safe and rich Rust wrapper around the Vulkan API. Vulkano is a Rust wrapper around [the Vulkan graphics API](https://www.vulkan.org/). It follows the Rust philosophy, which is that as long as you don't use unsafe code you shouldn't be able to trigger any undefined behavior. In the case of Vulkan, this means that non-unsafe code should always conform to valid API usage.

            - [Ash](https://github.com/ash-rs/ash) <img src="https://img.shields.io/github/stars/ash-rs/ash?style=social"/> : Vulkan bindings for Rust.

            - [ocl](https://github.com/cogciprocate/ocl) <img src="https://img.shields.io/github/stars/cogciprocate/ocl?style=social"/> : OpenCL for Rust.

            - [opencl3](https://github.com/kenba/opencl3) <img src="https://img.shields.io/github/stars/kenba/opencl3?style=social"/> : A Rust implementation of the Khronos [OpenCL 3.0](https://registry.khronos.org/OpenCL/) API.





        - ##### Julia Version

            - [CUDA.jl](https://github.com/JuliaGPU/CUDA.jl) <img src="https://img.shields.io/github/stars/JuliaGPU/CUDA.jl?style=social"/> : CUDA programming in Julia. [juliagpu.org/](https://juliagpu.org/)

            - [AMDGPU.jl](https://github.com/JuliaGPU/AMDGPU.jl) <img src="https://img.shields.io/github/stars/JuliaGPU/AMDGPU.jl?style=social"/> : AMD GPU (ROCm) programming in Julia.


    - #### Performance Benchmark

        - [NVIDIA/nvbench](https://github.com/NVIDIA/nvbench) <img src="https://img.shields.io/github/stars/NVIDIA/nvbench?style=social"/> : CUDA Kernel Benchmarking Library.

        - [FlagPerf](https://github.com/FlagOpen/FlagPerf) <img src="https://img.shields.io/github/stars/FlagOpen/FlagPerf?style=social"/> : FlagPerf is an open-source software platform for benchmarking AI chips. FlagPerfæ˜¯æ™ºæºç ”ç©¶é™¢è”åˆAIç¡¬ä»¶å‚å•†å…±å»ºçš„ä¸€ä½“åŒ–AIç¡¬ä»¶è¯„æµ‹å¼•æ“ï¼Œæ—¨åœ¨å»ºç«‹ä»¥äº§ä¸šå®è·µä¸ºå¯¼å‘çš„æŒ‡æ ‡ä½“ç³»ï¼Œè¯„æµ‹AIç¡¬ä»¶åœ¨è½¯ä»¶æ ˆç»„åˆï¼ˆæ¨¡å‹+æ¡†æ¶+ç¼–è¯‘å™¨ï¼‰ä¸‹çš„å®é™…èƒ½åŠ›ã€‚

        - [te42kyfo/gpu-benches](https://github.com/te42kyfo/gpu-benches) <img src="https://img.shields.io/github/stars/te42kyfo/gpu-benches?style=social"/> : collection of benchmarks to measure basic GPU capabilities.



    - #### Scientific Computing Framework
      ##### ç§‘å­¦è®¡ç®—æ¡†æ¶

        - [cuBLAS](https://developer.nvidia.com/cublas) : Basic Linear Algebra on NVIDIA GPUs. NVIDIA cuBLAS is a GPU-accelerated library for accelerating AI and HPC applications. It includes several API extensions for providing drop-in industry standard BLAS APIs and GEMM APIs with support for fusions that are highly optimized for NVIDIA GPUs. The cuBLAS library also contains extensions for batched operations, execution across multiple GPUs, and mixed- and low-precision execution with additional tuning for the best performance.

        - [CUTLASS](https://github.com/NVIDIA/cutlass) <img src="https://img.shields.io/github/stars/NVIDIA/cutlass?style=social"/> : CUDA Templates for Linear Algebra Subroutines.

        - [MatX](https://github.com/NVIDIA/MatX) <img src="https://img.shields.io/github/stars/NVIDIA/MatX?style=social"/> : MatX - GPU-Accelerated Numerical Computing in Modern C++. An efficient C++17 GPU numerical computing library with Python-like syntax. [nvidia.github.io/MatX](https://nvidia.github.io/MatX)

        - [DeepGEMM](https://github.com/deepseek-ai/DeepGEMM) <img src="https://img.shields.io/github/stars/deepseek-ai/DeepGEMM?style=social"/> : DeepGEMM: clean and efficient FP8 GEMM kernels with fine-grained scaling.

        - [MUTLASS](https://github.com/MooreThreads/mutlass) <img src="https://img.shields.io/github/stars/MooreThreads/mutlass?style=social"/> : MUSA Templates for Linear Algebra Subroutines.

        - [CuPy](https://github.com/cupy/cupy) <img src="https://img.shields.io/github/stars/cupy/cupy?style=social"/> : CuPy : NumPy & SciPy for GPU. [cupy.dev](https://cupy.dev/)

        - [GenericLinearAlgebra.jl](https://github.com/JuliaLinearAlgebra/GenericLinearAlgebra.jl) <img src="https://img.shields.io/github/stars/JuliaLinearAlgebra/GenericLinearAlgebra.jl?style=social"/> : Generic numerical linear algebra in Julia.

        - [custos-math](https://github.com/elftausend/custos-math) <img src="https://img.shields.io/github/stars/elftausend/custos-math?style=social"/> : This crate provides CUDA, OpenCL, CPU (and Stack) based matrix operations using [custos](https://github.com/elftausend/custos).


    - #### Attention and Transformer Framework

        - [FlashAttention](https://github.com/Dao-AILab/flash-attention) <img src="https://img.shields.io/github/stars/Dao-AILab/flash-attention?style=social"/> : Fast and memory-efficient exact attention. "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness". (**[arXiv 2022](https://arxiv.org/abs/2205.14135)**).

        - [SageAttention](https://github.com/thu-ml/SageAttention) <img src="https://img.shields.io/github/stars/thu-ml/SageAttention?style=social"/> : Quantized Attention that achieves speedups of 2.1-3.1x and 2.7-5.1x compared to FlashAttention2 and xformers, respectively, without lossing end-to-end metrics across various models. "SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration". (**[arXiv 2024](https://arxiv.org/abs/2410.02367)**). "SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization". (**[arXiv 2024](https://arxiv.org/abs/2411.10958)**).

        - [fla-org/flash-linear-attention](https://github.com/fla-org/flash-linear-attention) <img src="https://img.shields.io/github/stars/fla-org/flash-linear-attention?style=social"/> : ğŸš€ Efficient implementations of state-of-the-art linear attention models in Pytorch and Triton.

        - [66RING/tiny-flash-attention](https://github.com/66RING/tiny-flash-attention) <img src="https://img.shields.io/github/stars/66RING/tiny-flash-attention?style=social"/> : [flash attention](https://github.com/Dao-AILab/flash-attention) tutorial written in python, triton, cuda, cutlass.

        - [weishengying/tiny-flash-attention](https://github.com/weishengying/tiny-flash-attention) <img src="https://img.shields.io/github/stars/weishengying/tiny-flash-attention?style=social"/> : ä½¿ç”¨ cutlass å®ç° flash-attention ç²¾ç®€ç‰ˆï¼Œå…·æœ‰æ•™å­¦æ„ä¹‰ã€‚

        - [jepeake/tiny-flash-attention](https://github.com/jepeake/tiny-flash-attention) <img src="https://img.shields.io/github/stars/jepeake/tiny-flash-attention?style=social"/> : flash attention in ~20 lines.




    - #### Machine Learning Framework

        - [cuDNN](https://developer.nvidia.com/cudnn) : The NVIDIA CUDAÂ® Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for [deep neural networks](https://developer.nvidia.com/deep-learning). cuDNN provides highly tuned implementations for standard routines such as forward and backward convolution, attention, matmul, pooling, and normalization.

        - [PyTorch](https://github.com/pytorch/pytorch) <img src="https://img.shields.io/github/stars/pytorch/pytorch?style=social"/> : Tensors and Dynamic neural networks in Python with strong GPU acceleration. [pytorch.org](https://pytorch.org/)

        - [MooreThreads/torch_musa](https://github.com/MooreThreads/torch_musa) <img src="https://img.shields.io/github/stars/MooreThreads/torch_musa?style=social"/> : torch_musa is an open source repository based on PyTorch, which can make full use of the super computing power of MooreThreads graphics cards.

        - [PaddlePaddle](https://github.com/PaddlePaddle/Paddle) <img src="https://img.shields.io/github/stars/PaddlePaddle/Paddle?style=social"/> : PArallel Distributed Deep LEarning: Machine Learning Framework from Industrial Practice ï¼ˆã€é£æ¡¨ã€æ ¸å¿ƒæ¡†æ¶ï¼Œæ·±åº¦å­¦ä¹ &æœºå™¨å­¦ä¹ é«˜æ€§èƒ½å•æœºã€åˆ†å¸ƒå¼è®­ç»ƒå’Œè·¨å¹³å°éƒ¨ç½²ï¼‰. [www.paddlepaddle.org/](http://www.paddlepaddle.org/)

        - [flashlight/flashlight](https://github.com/flashlight/flashlight) <img src="https://img.shields.io/github/stars/flashlight/flashlight?style=social"/> : A C++ standalone library for machine learning. [fl.readthedocs.io/en/latest/](https://fl.readthedocs.io/en/latest/)

        - [yhwang-hub/dl_model_infer](https://github.com/yhwang-hub/dl_model_infer) <img src="https://img.shields.io/github/stars/yhwang-hub/dl_model_infer?style=social"/> : his is a c++ version of the AI reasoning library. Currently, it only supports the reasoning of the tensorrt model. The follow-up plan supports the c++ reasoning of frameworks such as Openvino, NCNN, and MNN. There are two versions for pre- and post-processing, c++ version and cuda version. It is recommended to use the cuda version., This repository provides accelerated deployment cases of deep learning CV popular models, and cuda c supports dynamic-batch image process, infer, decode, NMS.

        - [NVlabs/tiny-cuda-nn](https://github.com/NVlabs/tiny-cuda-nn) <img src="https://img.shields.io/github/stars/NVlabs/tiny-cuda-nn?style=social"/> : Lightning fast C++/CUDA neural network framework.

        - [zjhellofss/KuiperLLama](https://github.com/zjhellofss/KuiperLLama) <img src="https://img.shields.io/github/stars/zjhellofss/KuiperLLama?style=social"/> : ã€ŠåŠ¨æ‰‹è‡ªåˆ¶å¤§æ¨¡å‹æ¨ç†æ¡†æ¶ã€‹ã€‚KuiperLLama åŠ¨æ‰‹è‡ªåˆ¶å¤§æ¨¡å‹æ¨ç†æ¡†æ¶ï¼Œæ”¯æŒLLama2/3å’ŒQwen2.5ã€‚æ ¡æ‹›ã€ç§‹æ‹›ã€æ˜¥æ‹›ã€å®ä¹ å¥½é¡¹ç›®ï¼Œå¸¦ä½ ä»é›¶åŠ¨æ‰‹å®ç°æ”¯æŒLLama2/3å’ŒQwen2.5çš„å¤§æ¨¡å‹æ¨ç†æ¡†æ¶ã€‚

        - [zjhellofss/KuiperInfer](https://github.com/zjhellofss/KuiperInfer) <img src="https://img.shields.io/github/stars/zjhellofss/KuiperInfer?style=social"/> :  æ ¡æ‹›ã€ç§‹æ‹›ã€æ˜¥æ‹›ã€å®ä¹ å¥½é¡¹ç›®ï¼å¸¦ä½ ä»é›¶å®ç°ä¸€ä¸ªé«˜æ€§èƒ½çš„æ·±åº¦å­¦ä¹ æ¨ç†åº“ï¼Œæ”¯æŒå¤§æ¨¡å‹ llama2 ã€Unetã€Yolov5ã€Resnetç­‰æ¨¡å‹çš„æ¨ç†ã€‚Implement a high-performance deep learning inference library step by stepã€‚

        - [zjhellofss/kuiperdatawhale](https://github.com/zjhellofss/kuiperdatawhale) <img src="https://img.shields.io/github/stars/zjhellofss/kuiperdatawhale?style=social"/> :  ä»é›¶è‡ªåˆ¶æ·±åº¦å­¦ä¹ æ¨ç†æ¡†æ¶ã€‚

        - [MarioSieg/magnetron](https://github.com/MarioSieg/magnetron) <img src="https://img.shields.io/github/stars/MarioSieg/magnetron?style=social"/> :  (WIP) A small but powerful, homemade PyTorch from scratch. Minimalistic homemade PyTorch alternative, written in C99 and Python.

        - [lucasdelimanogueira/PyNorch](https://github.com/lucasdelimanogueira/PyNorch) <img src="https://img.shields.io/github/stars/lucasdelimanogueira/PyNorch?style=social"/> :  Recreating PyTorch from scratch (C/C++, CUDA, NCCL and Python, with multi-GPU support and automatic differentiation!)


    - #### AI Inference Framework
      ##### AIæ¨ç†æ¡†æ¶





        - ##### LLM Inference and Serving Engine

            - [TensorRT](https://github.com/NVIDIA/TensorRT) <img src="https://img.shields.io/github/stars/NVIDIA/TensorRT?style=social"/> : NVIDIAÂ® TensorRTâ„¢ is an SDK for high-performance deep learning inference on NVIDIA GPUs. This repository contains the open source components of TensorRT. [developer.nvidia.com/tensorrt](https://developer.nvidia.com/tensorrt)

            - [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) <img src="https://img.shields.io/github/stars/NVIDIA/TensorRT-LLM?style=social"/> : TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that execute those TensorRT engines. [nvidia.github.io/TensorRT-LLM](https://nvidia.github.io/TensorRT-LLM)

            - [NVIDIA/TensorRT-Model-Optimizer](https://github.com/NVIDIA/TensorRT-Model-Optimizer) <img src="https://img.shields.io/github/stars/NVIDIA/TensorRT-Model-Optimizer?style=social"/> : TensorRT Model Optimizer is a unified library of state-of-the-art model optimization techniques such as quantization, pruning, distillation, etc. It compresses deep learning models for downstream deployment frameworks like TensorRT-LLM or TensorRT to optimize inference speed on NVIDIA GPUs. [nvidia.github.io/TensorRT-Model-Optimizer](https://nvidia.github.io/TensorRT-Model-Optimizer/)

            - [Ollama](https://github.com/ollama/ollama) <img src="https://img.shields.io/github/stars/ollama/ollama?style=social"/> : Get up and running with Llama 3.3, DeepSeek-R1, Phi-4, Gemma 2, and other large language models. [ollama.com](https://ollama.com/)

            - [vLLM](https://github.com/vllm-project/vllm) <img src="https://img.shields.io/github/stars/vllm-project/vllm?style=social"/> : A high-throughput and memory-efficient inference and serving engine for LLMs. [docs.vllm.ai](https://docs.vllm.ai/)

            - [SGLang](https://github.com/sgl-project/sglang) <img src="https://img.shields.io/github/stars/sgl-project/sglang?style=social"/> : SGLang is a fast serving framework for large language models and vision language models. [docs.sglang.ai/](https://docs.sglang.ai/)

            - [MLC LLM](https://github.com/mlc-ai/mlc-llm) <img src="https://img.shields.io/github/stars/mlc-ai/mlc-llm?style=social"/> : Universal LLM Deployment Engine with ML Compilation. [llm.mlc.ai/](https://llm.mlc.ai/)

            - [KTransformers](https://github.com/kvcache-ai/ktransformers) <img src="https://img.shields.io/github/stars/kvcache-ai/ktransformers?style=social"/> : A Flexible Framework for Experiencing Cutting-edge LLM Inference Optimizations. [kvcache-ai.github.io/ktransformers/](https://kvcache-ai.github.io/ktransformers/)

            - [Chituï¼ˆèµ¤å…”ï¼‰](https://github.com/thu-pacman/chitu) <img src="https://img.shields.io/github/stars/thu-pacman/chitu?style=social"/> : High-performance inference framework for large language models, focusing on efficiency, flexibility, and availability.

            - [Aphrodite](https://github.com/aphrodite-engine/aphrodite-engine) <img src="https://img.shields.io/github/stars/aphrodite-engine/aphrodite-engine?style=social"/> : Large-scale LLM inference engine. [aphrodite.pygmalion.chat](https://aphrodite.pygmalion.chat/)

            - [GPUStack](https://github.com/gpustack/gpustack) <img src="https://img.shields.io/github/stars/gpustack/gpustack?style=social"/> : GPUStack is an open-source GPU cluster manager for running AI models. Manage GPU clusters for running AI models. [gpustack.ai](https://gpustack.ai/)

            - [Lamini](https://github.com/lamini-ai/lamini) <img src="https://img.shields.io/github/stars/lamini-ai/lamini?style=social"/> : The Official Python Client for Lamini's API. [lamini.ai/](https://lamini.ai/)

            - [datawhalechina/self-llm](https://github.com/datawhalechina/self-llm) <img src="https://img.shields.io/github/stars/datawhalechina/self-llm?style=social"/> :  ã€Šå¼€æºå¤§æ¨¡å‹é£Ÿç”¨æŒ‡å—ã€‹åŸºäºLinuxç¯å¢ƒå¿«é€Ÿéƒ¨ç½²å¼€æºå¤§æ¨¡å‹ï¼Œæ›´é€‚åˆä¸­å›½å®å®çš„éƒ¨ç½²æ•™ç¨‹ã€‚

            - [ninehills/llm-inference-benchmark](https://github.com/ninehills/llm-inference-benchmark) <img src="https://img.shields.io/github/stars/ninehills/llm-inference-benchmark?style=social"/> : LLM Inference benchmark.

            - [csbench/csbench](https://github.com/csbench/csbench) <img src="https://img.shields.io/github/stars/csbench/csbench?style=social"/> : "CS-Bench: A Comprehensive Benchmark for Large Language Models towards Computer Science Mastery". (**[arXiv 2024](https://arxiv.org/abs/2406.08587)**).

            - [MooreThreads/vllm_musa](https://github.com/MooreThreads/vllm_musa) <img src="https://img.shields.io/github/stars/MooreThreads/vllm_musa?style=social"/> : A high-throughput and memory-efficient inference and serving engine for LLMs. [docs.vllm.ai](https://docs.vllm.ai/)

            - [dusty-nv/jetson-inference](https://github.com/dusty-nv/jetson-inference) <img src="https://img.shields.io/github/stars/dusty-nv/jetson-inference?style=social"/> : Hello AI World guide to deploying deep-learning inference networks and deep vision primitives with TensorRT and NVIDIA Jetson.

            - [m0dulo/InferSpore](https://github.com/m0dulo/InferSpore) <img src="https://img.shields.io/github/stars/m0dulo/InferSpore?style=social"/> : ğŸŒ± A fully independent Large Language Model (LLM) inference engine, built leveraging cuBLAS and cub. ğŸ§©




        - ##### High Performance Kernel Library

            - [DeepGEMM](https://github.com/deepseek-ai/DeepGEMM) <img src="https://img.shields.io/github/stars/deepseek-ai/DeepGEMM?style=social"/> : DeepGEMM: clean and efficient FP8 GEMM kernels with fine-grained scaling.

            - [FlashInfer](https://github.com/flashinfer-ai/flashinfer) <img src="https://img.shields.io/github/stars/flashinfer-ai/flashinfer?style=social"/> : FlashInfer: Kernel Library for LLM Serving . [flashinfer.ai](flashinfer.ai)

            - [FlashMLA](https://github.com/deepseek-ai/FlashMLA) <img src="https://img.shields.io/github/stars/deepseek-ai/FlashMLA?style=social"/> : FlashMLA: Efficient MLA Decoding Kernel for Hopper GPUs.

            - [DeepEP](https://github.com/deepseek-ai/DeepEP) <img src="https://img.shields.io/github/stars/deepseek-ai/DeepEP?style=social"/> : DeepEP: an efficient expert-parallel communication library.




        - ##### C Implementation

            - [llm.c](https://github.com/karpathy/llm.c) <img src="https://img.shields.io/github/stars/karpathy/llm.c?style=social"/> : LLM training in simple, pure C/CUDA. There is no need for 245MB of PyTorch or 107MB of cPython. For example, training GPT-2 (CPU, fp32) is ~1,000 lines of clean code in a single file. It compiles and runs instantly, and exactly matches the PyTorch reference implementation.

            - [llama2.c](https://github.com/karpathy/llama2.c) <img src="https://img.shields.io/github/stars/karpathy/llama2.c?style=social"/> : Inference Llama 2 in one file of pure C. Train the Llama 2 LLM architecture in PyTorch then inference it with one simple 700-line C file (run.c).


        - ##### CPP Implementation

            - [gemma.cpp](https://github.com/google/gemma.cpp) <img src="https://img.shields.io/github/stars/google/gemma.cpp?style=social"/> :  gemma.cpp is a lightweight, standalone C++ inference engine for the Gemma foundation models from Google.

            - [llama.cpp](https://github.com/ggerganov/llama.cpp) <img src="https://img.shields.io/github/stars/ggerganov/llama.cpp?style=social"/> : Inference of [LLaMA](https://github.com/facebookresearch/llama) model in pure C/C++.

            - [whisper.cpp](https://github.com/ggerganov/whisper.cpp) <img src="https://img.shields.io/github/stars/ggerganov/whisper.cpp?style=social"/> : High-performance inference of [OpenAI's Whisper](https://github.com/openai/whisper) automatic speech recognition (ASR) model.

            - [ChatGLM.cpp](https://github.com/li-plus/chatglm.cpp) <img src="https://img.shields.io/github/stars/li-plus/chatglm.cpp?style=social"/> : C++ implementation of [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) and [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B).

            - [MegEngine/InferLLM](https://github.com/MegEngine/InferLLM) <img src="https://img.shields.io/github/stars/MegEngine/InferLLM?style=social"/> : InferLLM is a lightweight LLM model inference framework that mainly references and borrows from the llama.cpp project.

            - [DeployAI/nndeploy](https://github.com/DeployAI/nndeploy) <img src="https://img.shields.io/github/stars/DeployAI/nndeploy?style=social"/> : nndeployæ˜¯ä¸€æ¬¾æ¨¡å‹ç«¯åˆ°ç«¯éƒ¨ç½²æ¡†æ¶ã€‚ä»¥å¤šç«¯æ¨ç†ä»¥åŠåŸºäºæœ‰å‘æ— ç¯å›¾æ¨¡å‹éƒ¨ç½²ä¸ºå†…æ ¸ï¼Œè‡´åŠ›ä¸ºç”¨æˆ·æä¾›è·¨å¹³å°ã€ç®€å•æ˜“ç”¨ã€é«˜æ€§èƒ½çš„æ¨¡å‹éƒ¨ç½²ä½“éªŒã€‚[nndeploy-zh.readthedocs.io/zh/latest/](https://nndeploy-zh.readthedocs.io/zh/latest/)

            - [zjhellofss/KuiperInfer (è‡ªåˆ¶æ·±åº¦å­¦ä¹ æ¨ç†æ¡†æ¶)](https://github.com/zjhellofss/KuiperInfer) <img src="https://img.shields.io/github/stars/zjhellofss/KuiperInfer?style=social"/> :  å¸¦ä½ ä»é›¶å®ç°ä¸€ä¸ªé«˜æ€§èƒ½çš„æ·±åº¦å­¦ä¹ æ¨ç†åº“ï¼Œæ”¯æŒllama ã€Unetã€Yolov5ã€Resnetç­‰æ¨¡å‹çš„æ¨ç†ã€‚Implement a high-performance deep learning inference library step by step.

            - [skeskinen/llama-lite](https://github.com/skeskinen/llama-lite) <img src="https://img.shields.io/github/stars/skeskinen/llama-lite?style=social"/> : Embeddings focused small version of Llama NLP model.

            - [Const-me/Whisper](https://github.com/Const-me/Whisper) <img src="https://img.shields.io/github/stars/Const-me/Whisper?style=social"/> : High-performance GPGPU inference of OpenAI's Whisper automatic speech recognition (ASR) model.

            - [wangzhaode/ChatGLM-MNN](https://github.com/wangzhaode/ChatGLM-MNN) <img src="https://img.shields.io/github/stars/wangzhaode/ChatGLM-MNN?style=social"/> : Pure C++, Easy Deploy ChatGLM-6B.

            - [ztxz16/fastllm](https://github.com/ztxz16/fastllm) <img src="https://img.shields.io/github/stars/ztxz16/fastllm?style=social"/> : çº¯c++å®ç°ï¼Œæ— ç¬¬ä¸‰æ–¹ä¾èµ–çš„å¤§æ¨¡å‹åº“ï¼Œæ”¯æŒCUDAåŠ é€Ÿï¼Œç›®å‰æ”¯æŒå›½äº§å¤§æ¨¡å‹ChatGLM-6Bï¼ŒMOSS; å¯ä»¥åœ¨å®‰å“è®¾å¤‡ä¸Šæµç•…è¿è¡ŒChatGLM-6Bã€‚

            - [davidar/eigenGPT](https://github.com/davidar/eigenGPT) <img src="https://img.shields.io/github/stars/davidar/eigenGPT?style=social"/> : Minimal C++ implementation of GPT2.

            - [Tlntin/Qwen-TensorRT-LLM](https://github.com/Tlntin/Qwen-TensorRT-LLM) <img src="https://img.shields.io/github/stars/Tlntin/Qwen-TensorRT-LLM?style=social"/> : ä½¿ç”¨TRT-LLMå®Œæˆå¯¹Qwen-7B-Chatå®ç°æ¨ç†åŠ é€Ÿã€‚

            - [FeiGeChuanShu/trt2023](https://github.com/FeiGeChuanShu/trt2023) <img src="https://img.shields.io/github/stars/FeiGeChuanShu/trt2023?style=social"/> : NVIDIA TensorRT Hackathon 2023å¤èµ›é€‰é¢˜ï¼šé€šä¹‰åƒé—®Qwen-7Bç”¨TensorRT-LLMæ¨¡å‹æ­å»ºåŠä¼˜åŒ–ã€‚

            - [TRT2022/trtllm-llama](https://github.com/TRT2022/trtllm-llama) <img src="https://img.shields.io/github/stars/TRT2022/trtllm-llama?style=social"/> : â˜¢ï¸ TensorRT 2023å¤èµ›â€”â€”åŸºäºTensorRT-LLMçš„Llamaæ¨¡å‹æ¨æ–­åŠ é€Ÿä¼˜åŒ–ã€‚



        - ##### Mojo Implementation

            - [llama2.mojo](https://github.com/tairov/llama2.mojo) <img src="https://img.shields.io/github/stars/tairov/llama2.mojo?style=social"/> : Inference Llama 2 in one file of pure ğŸ”¥

            - [dorjeduck/llm.mojo](https://github.com/dorjeduck/llm.mojo) <img src="https://img.shields.io/github/stars/dorjeduck/llm.mojo?style=social"/> : port of Andrjey Karpathy's llm.c to Mojo.


        - ##### Rust Implementation

            - [Candle](https://github.com/huggingface/candle) <img src="https://img.shields.io/github/stars/huggingface/candle?style=social"/> : Minimalist ML framework for Rust.

            - [Safetensors](https://github.com/huggingface/safetensors) <img src="https://img.shields.io/github/stars/huggingface/safetensors?style=social"/> : Simple, safe way to store and distribute tensors. [huggingface.co/docs/safetensors](https://huggingface.co/docs/safetensors/index)

            - [Tokenizers](https://github.com/huggingface/tokenizers) <img src="https://img.shields.io/github/stars/huggingface/tokenizers?style=social"/> : ğŸ’¥ Fast State-of-the-Art Tokenizers optimized for Research and Production. [huggingface.co/docs/tokenizers](https://huggingface.co/docs/tokenizers/index)

            - [Burn](https://github.com/burn-rs/burn) <img src="https://img.shields.io/github/stars/burn-rs/burn?style=social"/> : Burn - A Flexible and Comprehensive Deep Learning Framework in Rust. [burn-rs.github.io/](https://burn-rs.github.io/)

            - [dfdx](https://github.com/coreylowman/dfdx) <img src="https://img.shields.io/github/stars/coreylowman/dfdx?style=social"/> : Deep learning in Rust, with shape checked tensors and neural networks.

            - [luminal](https://github.com/jafioti/luminal) <img src="https://img.shields.io/github/stars/jafioti/luminal?style=social"/> : Deep learning at the speed of light. [www.luminalai.com/](https://www.luminalai.com/)

            - [crabml](https://github.com/crabml/crabml) <img src="https://img.shields.io/github/stars/crabml/crabml?style=social"/> : crabml is focusing on the reimplementation of GGML using the Rust programming language.

            - [TensorFlow Rust](https://github.com/tensorflow/rust) <img src="https://img.shields.io/github/stars/tensorflow/rust?style=social"/> : Rust language bindings for TensorFlow.

            - [tch-rs](https://github.com/LaurentMazare/tch-rs) <img src="https://img.shields.io/github/stars/LaurentMazare/tch-rs?style=social"/> : Rust bindings for the C++ api of PyTorch.

            - [rustai-solutions/candle_demo_openchat_35](https://github.com/rustai-solutions/candle_demo_openchat_35) <img src="https://img.shields.io/github/stars/rustai-solutions/candle_demo_openchat_35?style=social"/> : candle_demo_openchat_35.

            - [llama2.rs](https://github.com/srush/llama2.rs) <img src="https://img.shields.io/github/stars/srush/llama2.rs?style=social"/> : A fast llama2 decoder in pure Rust.

            - [Llama2-burn](https://github.com/Gadersd/llama2-burn) <img src="https://img.shields.io/github/stars/Gadersd/llama2-burn?style=social"/> : Llama2 LLM ported to Rust burn.

            - [gaxler/llama2.rs](https://github.com/gaxler/llama2.rs) <img src="https://img.shields.io/github/stars/gaxler/llama2.rs?style=social"/> : Inference Llama 2 in one file of pure Rust ğŸ¦€

            - [whisper-burn](https://github.com/Gadersd/whisper-burn) <img src="https://img.shields.io/github/stars/Gadersd/whisper-burn?style=social"/> : A Rust implementation of OpenAI's Whisper model using the burn framework.

            - [stable-diffusion-burn](https://github.com/Gadersd/stable-diffusion-burn) <img src="https://img.shields.io/github/stars/Gadersd/stable-diffusion-burn?style=social"/> : Stable Diffusion v1.4 ported to Rust's burn framework.

            - [coreylowman/llama-dfdx](https://github.com/coreylowman/llama-dfdx) <img src="https://img.shields.io/github/stars/coreylowman/llama-dfdx?style=social"/> : [LLaMa 7b](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) with CUDA acceleration implemented in rust. Minimal GPU memory needed!

            - [tazz4843/whisper-rs](https://github.com/tazz4843/whisper-rs) <img src="https://img.shields.io/github/stars/tazz4843/whisper-rs?style=social"/> : Rust bindings to [whisper.cpp](https://github.com/ggerganov/whisper.cpp).

            - [rustformers/llm](https://github.com/rustformers/llm) <img src="https://img.shields.io/github/stars/rustformers/llm?style=social"/> : Run inference for Large Language Models on CPU, with Rust ğŸ¦€ğŸš€ğŸ¦™.

            - [Chidori](https://github.com/ThousandBirdsInc/chidori) <img src="https://img.shields.io/github/stars/ThousandBirdsInc/chidori?style=social"/> : A reactive runtime for building durable AI agents. [docs.thousandbirds.ai](https://docs.thousandbirds.ai/).

            - [llm-chain](https://github.com/sobelio/llm-chain) <img src="https://img.shields.io/github/stars/sobelio/llm-chain?style=social"/> : llm-chain is a collection of Rust crates designed to help you work with Large Language Models (LLMs) more effectively. [llm-chain.xyz](https://llm-chain.xyz/)

            - [Atome-FE/llama-node](https://github.com/Atome-FE/llama-node) <img src="https://img.shields.io/github/stars/Atome-FE/llama-node?style=social"/> : Believe in AI democratization. llama for nodejs backed by llama-rs and llama.cpp, work locally on your laptop CPU. support llama/alpaca/gpt4all/vicuna model. [www.npmjs.com/package/llama-node](https://www.npmjs.com/package/llama-node)

            - [Noeda/rllama](https://github.com/Noeda/rllama) <img src="https://img.shields.io/github/stars/Noeda/rllama?style=social"/> : Rust+OpenCL+AVX2 implementation of LLaMA inference code.

            - [lencx/ChatGPT](https://github.com/lencx/ChatGPT) <img src="https://img.shields.io/github/stars/lencx/ChatGPT?style=social"/> : ğŸ”® ChatGPT Desktop Application (Mac, Windows and Linux). [NoFWL](https://app.nofwl.com/).

            - [Synaptrix/ChatGPT-Desktop](https://github.com/Synaptrix/ChatGPT-Desktop) <img src="https://img.shields.io/github/stars/Synaptrix/ChatGPT-Desktop?style=social"/> : Fuel your productivity with ChatGPT-Desktop - Blazingly fast and supercharged!

            - [Poordeveloper/chatgpt-app](https://github.com/Poordeveloper/chatgpt-app) <img src="https://img.shields.io/github/stars/Poordeveloper/chatgpt-app?style=social"/> : A ChatGPT App for all platforms. Built with Rust + Tauri + Vue + Axum.

            - [mxismean/chatgpt-app](https://github.com/mxismean/chatgpt-app) <img src="https://img.shields.io/github/stars/mxismean/chatgpt-app?style=social"/> : Tauri é¡¹ç›®ï¼šChatGPT App.

            - [sonnylazuardi/chat-ai-desktop](https://github.com/sonnylazuardi/chat-ai-desktop) <img src="https://img.shields.io/github/stars/sonnylazuardi/chat-ai-desktop?style=social"/> : Chat AI Desktop App. Unofficial ChatGPT desktop app for Mac & Windows menubar using Tauri & Rust.

            - [yetone/openai-translator](https://github.com/yetone/openai-translator) <img src="https://img.shields.io/github/stars/yetone/openai-translator?style=social"/> : The translator that does more than just translation - powered by OpenAI.

            - [m1guelpf/browser-agent](https://github.com/m1guelpf/browser-agent) <img src="https://img.shields.io/github/stars/m1guelpf/browser-agent?style=social"/> : A browser AI agent, using GPT-4. [docs.rs/browser-agent](https://docs.rs/browser-agent/latest/browser_agent/)

            - [sigoden/aichat](https://github.com/sigoden/aichat) <img src="https://img.shields.io/github/stars/sigoden/aichat?style=social"/> : Using ChatGPT/GPT-3.5/GPT-4 in the terminal.

            - [uiuifree/rust-openai-chatgpt-api](https://github.com/uiuifree/rust-openai-chatgpt-api) <img src="https://img.shields.io/github/stars/uiuifree/rust-openai-chatgpt-api?style=social"/> : "rust-openai-chatgpt-api" is a Rust library for accessing the ChatGPT API, a powerful NLP platform by OpenAI. The library provides a simple and efficient interface for sending requests and receiving responses, including chat. It uses reqwest and serde for HTTP requests and JSON serialization.

            - [1595901624/gpt-aggregated-edition](https://github.com/1595901624/gpt-aggregated-edition) <img src="https://img.shields.io/github/stars/1595901624/gpt-aggregated-edition?style=social"/> : èšåˆChatGPTå®˜æ–¹ç‰ˆã€ChatGPTå…è´¹ç‰ˆã€æ–‡å¿ƒä¸€è¨€ã€Poeã€chatchatç­‰å¤šå¹³å°ï¼Œæ”¯æŒè‡ªå®šä¹‰å¯¼å…¥å¹³å°ã€‚

            - [Cormanz/smartgpt](https://github.com/Cormanz/smartgpt) <img src="https://img.shields.io/github/stars/Cormanz/smartgpt?style=social"/> : A program that provides LLMs with the ability to complete complex tasks using plugins.

            - [femtoGPT](https://github.com/keyvank/femtoGPT) <img src="https://img.shields.io/github/stars/keyvank/femtoGPT?style=social"/> : femtoGPT is a pure Rust implementation of a minimal Generative Pretrained Transformer. [discord.gg/wTJFaDVn45](https://github.com/keyvank/femtoGPT)

            - [shafishlabs/llmchain-rs](https://github.com/shafishlabs/llmchain-rs) <img src="https://img.shields.io/github/stars/shafishlabs/llmchain-rs?style=social"/> : ğŸ¦€Rust + Large Language Models - Make AI Services Freely and Easily. Inspired by LangChain.

            - [flaneur2020/llama2.rs](https://github.com/flaneur2020/llama2.rs) <img src="https://img.shields.io/github/stars/flaneur2020/llama2.rs?style=social"/> : An rust reimplementatin of [https://github.com/karpathy/llama2.c](https://github.com/karpathy/llama2.c).

            - [Heng30/chatbox](https://github.com/Heng30/chatbox) <img src="https://img.shields.io/github/stars/Heng30/chatbox?style=social"/> : A Chatbot for OpenAI ChatGPT. Based on Slint-ui and Rust.

            - [fairjm/dioxus-openai-qa-gui](https://github.com/fairjm/dioxus-openai-qa-gui) <img src="https://img.shields.io/github/stars/fairjm/dioxus-openai-qa-gui?style=social"/> : a simple openai qa desktop app built with dioxus.

            - [purton-tech/bionicgpt](https://github.com/purton-tech/bionicgpt) <img src="https://img.shields.io/github/stars/purton-tech/bionicgpt?style=social"/> : Accelerate LLM adoption in your organisation. Chat with your confidential data safely and securely. [bionic-gpt.com](https://bionic-gpt.com/)




        - #### Zig Implementation

            - [llama2.zig](https://github.com/cgbur/llama2.zig) <img src="https://img.shields.io/github/stars/cgbur/llama2.zig?style=social"/> : Inference Llama 2 in one file of pure Zig.

            - [renerocksai/gpt4all.zig](https://github.com/renerocksai/gpt4all.zig) <img src="https://img.shields.io/github/stars/renerocksai/gpt4all.zig?style=social"/> : ZIG build for a terminal-based chat client for an assistant-style large language model with ~800k GPT-3.5-Turbo Generations based on LLaMa.

            - [EugenHotaj/zig_inference](https://github.com/EugenHotaj/zig_inference) <img src="https://img.shields.io/github/stars/EugenHotaj/zig_inference?style=social"/> : Neural Network Inference Engine in Zig.


        - ##### Go Implementation

            - [Ollama](https://github.com/ollama/ollama/) <img src="https://img.shields.io/github/stars/ollama/ollama?style=social"/> : Get up and running with Llama 2, Mistral, Gemma, and other large language models. [ollama.com](https://ollama.com/)


            - [go-skynet/LocalAI](https://github.com/go-skynet/LocalAI) <img src="https://img.shields.io/github/stars/go-skynet/LocalAI?style=social"/> : ğŸ¤– Self-hosted, community-driven, local OpenAI-compatible API. Drop-in replacement for OpenAI running LLMs on consumer-grade hardware. Free Open Source OpenAI alternative. No GPU required. LocalAI is an API to run ggml compatible models: llama, gpt4all, rwkv, whisper, vicuna, koala, gpt4all-j, cerebras, falcon, dolly, starcoder, and many other. [localai.io](https://localai.io/)



    - #### Distributed and Multi-GPU Framework
      ##### åˆ†å¸ƒå¼ä»¥åŠå¤šGPUæ¡†æ¶

        - [NVIDIA/nccl](https://github.com/NVIDIA/nccl) <img src="https://img.shields.io/github/stars/NVIDIA/nccl?style=social"/> : Optimized primitives for collective multi-GPU communication.

        - [NVIDIA/multi-gpu-programming-models](https://github.com/NVIDIA/multi-gpu-programming-models) <img src="https://img.shields.io/github/stars/NVIDIA/multi-gpu-programming-models?style=social"/> : Examples demonstrating available options to program multiple GPUs in a single node or a cluster.

        - [wilicc/gpu-burn](https://github.com/wilicc/gpu-burn) <img src="https://img.shields.io/github/stars/wilicc/gpu-burn?style=social"/> : Multi-GPU CUDA stress test.

        - [SCUDA](https://github.com/kevmo314/scuda) <img src="https://img.shields.io/github/stars/kevmo314/scuda?style=social"/> : SCUDA: GPU-over-IP. SCUDA is a GPU over IP bridge allowing GPUs on remote machines to be attached to CPU-only machines.




    - #### Robotics Framework
      ##### æœºå™¨äººæ¡†æ¶


        - [Cupoch](https://github.com/neka-nat/cupoch) <img src="https://img.shields.io/github/stars/neka-nat/cupoch?style=social"/> : Robotics with GPU computing.



    - #### ZKP and Web3 Framework
      ##### é›¶çŸ¥è¯†è¯æ˜å’ŒWeb3æ¡†æ¶

        - [Tachyon](https://github.com/kroma-network/tachyon) <img src="https://img.shields.io/github/stars/kroma-network/tachyon?style=social"/> : Modular ZK(Zero Knowledge) backend accelerated by GPU.

        - [Blitzar](https://github.com/spaceandtimelabs/blitzar) <img src="https://img.shields.io/github/stars/spaceandtimelabs/blitzar?style=social"/> : Zero-knowledge proof acceleration with GPUs for C++ and Rust. [www.spaceandtime.io/](https://www.spaceandtime.io/)

        - [blitzar-rs](https://github.com/spaceandtimelabs/blitzar-rs) <img src="https://img.shields.io/github/stars/spaceandtimelabs/blitzar-rs?style=social"/> : High-Level Rust wrapper for the blitzar-sys crate. [www.spaceandtime.io/](https://www.spaceandtime.io/)

        - [ICICLE](https://github.com/ingonyama-zk/icicle) <img src="https://img.shields.io/github/stars/ingonyama-zk/icicle?style=social"/> : ICICLE is a library for ZK acceleration using CUDA-enabled GPUs.




  - ### Triton Frameworks

    - #### Triton Machine Learning Framework

        - [BobMcDear/attorch](https://github.com/BobMcDear/attorch) <img src="https://img.shields.io/github/stars/BobMcDear/attorch?style=social"/> : A subset of PyTorch's neural network modules, written in Python using OpenAI's Triton.



    - #### Triton High Performance Kernel Library

        - [Liger-Kernel](https://github.com/linkedin/Liger-Kernel) <img src="https://img.shields.io/github/stars/linkedin/Liger-Kernel?style=social"/> : Efficient Triton Kernels for LLM Training. [arxiv.org/pdf/2410.10989](https://arxiv.org/pdf/2410.10989)

        - [FlagGems](https://github.com/FlagOpen/FlagGems) <img src="https://img.shields.io/github/stars/FlagOpen/FlagGems?style=social"/> : FlagGems is a high-performance general operator library implemented in [OpenAI Triton](https://github.com/openai/triton). It aims to provide a suite of kernel functions to accelerate LLM training and inference.

        - [linxihui/dkernel](https://github.com/linxihui/dkernel) <img src="https://img.shields.io/github/stars/linxihui/dkernel?style=social"/> : This repo contains customized CUDA kernels written in OpenAI Triton. As of now, it contains the sparse attention kernel used in [phi-3-small models](https://huggingface.co/microsoft/Phi-3-small-8k-instruct). The sparse attention is also supported in vLLM for efficient inference.



    - #### Triton Inference Framework

        - [harleyszhang/lite_llama](https://github.com/harleyszhang/lite_llama) <img src="https://img.shields.io/github/stars/harleyszhang/lite_llama?style=social"/> : The llama model inference lite framework by triton.


  - ### MLIR Frameworks

    - #### MLIR GPU Programming

        - ['gpu' Dialect](https://mlir.llvm.org/docs/Dialects/GPU/) : This dialect provides middle-level abstractions for launching GPU kernels following a programming model similar to that of CUDA or OpenCL.

        - ['amdgpu' Dialect](https://mlir.llvm.org/docs/Dialects/AMDGPU/) : The AMDGPU dialect provides wrappers around AMD-specific functionality and LLVM intrinsics.



    - #### MLIR FFI Bindings

        - [pyMLIR](https://github.com/spcl/pymlir) <img src="https://img.shields.io/github/stars/spcl/pymlir?style=social"/> : Python interface for MLIR - the Multi-Level Intermediate Representation. pyMLIR is a full Python interface to parse, process, and output [MLIR](https://mlir.llvm.org/) files according to the syntax described in the [MLIR documentation](https://github.com/llvm/llvm-project/tree/master/mlir/docs). pyMLIR supports the basic dialects and can be extended with other dialects.


    - #### MLIR Machine Learning Framework

        - [Torch-MLIR](https://github.com/llvm/torch-mlir) <img src="https://img.shields.io/github/stars/llvm/torch-mlir?style=social"/> : The Torch-MLIR project aims to provide first class support from the PyTorch ecosystem to the MLIR ecosystem.

        - [ONNX-MLIR](https://github.com/onnx/onnx-mlir) <img src="https://img.shields.io/github/stars/onnx/onnx-mlir?style=social"/> : Representation and Reference Lowering of ONNX Models in MLIR Compiler Infrastructure.

        - [TPU-MLIR](https://github.com/sophgo/tpu-mlir) <img src="https://img.shields.io/github/stars/sophgo/tpu-mlir?style=social"/> : Machine learning compiler based on MLIR for Sophgo TPU. TPU-MLIR is an open-source machine-learning compiler based on MLIR for TPU. This project provides a complete toolchain, which can convert pre-trained neural networks from different frameworks into binary files bmodel that can be efficiently operated on TPUs.

        - [IREE](https://github.com/iree-org/iree) <img src="https://img.shields.io/github/stars/iree-org/iree?style=social"/> : IREE: Intermediate Representation Execution Environment. A retargetable MLIR-based machine learning compiler and runtime toolkit. [iree.dev/](http://iree.dev/)

        - [ByteIR](https://github.com/bytedance/byteir) <img src="https://img.shields.io/github/stars/bytedance/byteir?style=social"/> : The ByteIR Project is a ByteDance model compilation solution. ByteIR includes compiler, runtime, and frontends, and provides an end-to-end model compilation solution. [byteir.ai](https://byteir.ai/)

        - [Xilinx/mlir-aie](https://github.com/Xilinx/mlir-aie) <img src="https://img.shields.io/github/stars/Xilinx/mlir-aie?style=social"/> : An MLIR-based toolchain for AMD AI Engine-enabled devices. This repository contains an MLIR-based toolchain for AI Engine-enabled devices, such as [AMD Ryzenâ„¢ AI](https://www.amd.com/en/products/processors/consumer/ryzen-ai.html) and [Versalâ„¢](https://www.xilinx.com/products/technology/ai-engine.html).







  - ### HPC Frameworks

    - [BLAS](https://www.netlib.org/blas/) : BLAS (Basic Linear Algebra Subprograms). The BLAS (Basic Linear Algebra Subprograms) are routines that provide standard building blocks for performing basic vector and matrix operations. The Level 1 BLAS perform scalar, vector and vector-vector operations, the Level 2 BLAS perform matrix-vector operations, and the Level 3 BLAS perform matrix-matrix operations.

    - [LAPACK](https://github.com/Reference-LAPACK/lapack) <img src="https://img.shields.io/github/stars/Reference-LAPACK/lapack?style=social"/> : LAPACK development repository. [LAPACK](https://www.netlib.org/lapack/)â€‰â€”â€‰Linear Algebra PACKage. LAPACK is written in Fortran 90 and provides routines for solving systems of simultaneous linear equations, least-squares solutions of linear systems of equations, eigenvalue problems, and singular value problems. The associated matrix factorizations (LU, Cholesky, QR, SVD, Schur, generalized Schur) are also provided, as are related computations such as reordering of the Schur factorizations and estimating condition numbers. Dense and banded matrices are handled, but not general sparse matrices. In all areas, similar functionality is provided for real and complex matrices, in both single and double precision.

    - [OpenBLAS](https://github.com/OpenMathLib/OpenBLAS) <img src="https://img.shields.io/github/stars/OpenMathLib/OpenBLAS?style=social"/> : OpenBLAS is an optimized BLAS library based on GotoBLAS2 1.13 BSD version. [www.openblas.net](http://www.openblas.net/)

    - [BLIS](https://github.com/flame/blis) <img src="https://img.shields.io/github/stars/flame/blis?style=social"/> : BLAS-like Library Instantiation Software Framework.

    - [NumPy](https://github.com/numpy/numpy) <img src="https://img.shields.io/github/stars/numpy/numpy?style=social"/> : The fundamental package for scientific computing with Python. [numpy.org](https://numpy.org/)

    - [SciPy](https://github.com/scipy/scipy) <img src="https://img.shields.io/github/stars/scipy/scipy?style=social"/> : SciPy library main repository. SciPy (pronounced "Sigh Pie") is an open-source software for mathematics, science, and engineering. It includes modules for statistics, optimization, integration, linear algebra, Fourier transforms, signal and image processing, ODE solvers, and more. [scipy.org](https://scipy.org/)

    - [Gonum](https://github.com/gonum/gonum) <img src="https://img.shields.io/github/stars/gonum/gonum?style=social"/> : Gonum is a set of numeric libraries for the Go programming language. It contains libraries for matrices, statistics, optimization, and more. [www.gonum.org/](https://www.gonum.org/)

    - [YichengDWu/matmul.mojo](https://github.com/YichengDWu/matmul.mojo) <img src="https://img.shields.io/github/stars/YichengDWu/matmul.mojo?style=social"/> : High Performance Matrix Multiplication in Pure Mojo ğŸ”¥. Matmul.ğŸ”¥ is a high performance muilti-threaded implimentation of the [BLIS](https://en.wikipedia.org/wiki/BLIS_(software)) algorithm in pure Mojo ğŸ”¥.




## Applications

  - ### CUDA Applications


    - #### Image Preprocess

        - [dusty-nv/jetson-utils](https://github.com/dusty-nv/jetson-utils) <img src="https://img.shields.io/github/stars/dusty-nv/jetson-utils?style=social"/> : C++/CUDA/Python multimedia utilities for NVIDIA Jetson.

        - [emptysoal/cuda-image-preprocess](https://github.com/emptysoal/cuda-image-preprocess) <img src="https://img.shields.io/github/stars/emptysoal/cuda-image-preprocess?style=social"/> : Speed up image preprocess with cuda when handle image or tensorrt inference. Cudaç¼–ç¨‹åŠ é€Ÿå›¾åƒé¢„å¤„ç†ã€‚



    - #### Object Detection

        - [laugh12321/TensorRT-YOLO](https://github.com/laugh12321/TensorRT-YOLO) <img src="https://img.shields.io/github/stars/laugh12321/TensorRT-YOLO?style=social"/> : ğŸš€ TensorRT-YOLO: Support YOLOv3, YOLOv5, YOLOv6, YOLOv7, YOLOv8, YOLOv9, YOLOv10, PP-YOLOE using TensorRT acceleration with EfficientNMS! TensorRT-YOLO æ˜¯ä¸€ä¸ªæ”¯æŒ YOLOv3ã€YOLOv5ã€YOLOv6ã€YOLOv7ã€YOLOv8ã€YOLOv9ã€YOLOv10ã€PP-YOLOE å’Œ PP-YOLOE+ çš„æ¨ç†åŠ é€Ÿé¡¹ç›®ï¼Œä½¿ç”¨ NVIDIA TensorRT è¿›è¡Œä¼˜åŒ–ã€‚é¡¹ç›®ä¸ä»…é›†æˆäº† EfficientNMS TensorRT æ’ä»¶ä»¥å¢å¼ºåå¤„ç†æ•ˆæœï¼Œè¿˜ä½¿ç”¨äº† CUDA æ ¸å‡½æ•°æ¥åŠ é€Ÿå‰å¤„ç†è¿‡ç¨‹ã€‚TensorRT-YOLO æä¾›äº† C++ å’Œ Python æ¨ç†çš„æ”¯æŒï¼Œæ—¨åœ¨æä¾›å¿«é€Ÿè€Œä¼˜åŒ–çš„ç›®æ ‡æ£€æµ‹è§£å†³æ–¹æ¡ˆã€‚

        - [l-sf/Linfer](https://github.com/l-sf/Linfer) <img src="https://img.shields.io/github/stars/l-sf/Linfer?style=social"/> : åŸºäºTensorRTçš„C++é«˜æ€§èƒ½æ¨ç†åº“ï¼ŒYolov10, YoloPv2ï¼ŒYolov5/7/X/8ï¼ŒRT-DETRï¼Œå•ç›®æ ‡è·Ÿè¸ªOSTrackã€LightTrackã€‚

        - [Melody-Zhou/tensorRT_Pro-YOLOv8](https://github.com/Melody-Zhou/tensorRT_Pro-YOLOv8) <img src="https://img.shields.io/github/stars/Melody-Zhou/tensorRT_Pro-YOLOv8?style=social"/> : This repository is based on [shouxieai/tensorRT_Pro](https://github.com/shouxieai/tensorRT_Pro), with adjustments to support YOLOv8. ç›®å‰å·²æ”¯æŒ YOLOv8ã€YOLOv8-Clsã€YOLOv8-Segã€YOLOv8-OBBã€YOLOv8-Poseã€RT-DETRã€ByteTrackã€YOLOv9ã€YOLOv10ã€RTMO é«˜æ€§èƒ½æ¨ç†ï¼ï¼ï¼ğŸš€ğŸš€ğŸš€

        - [shouxieai/tensorRT_Pro](https://github.com/shouxieai/tensorRT_Pro) <img src="https://img.shields.io/github/stars/shouxieai/tensorRT_Pro?style=social"/> : C++ library based on tensorrt integration.

        - [shouxieai/infer](https://github.com/shouxieai/infer) <img src="https://img.shields.io/github/stars/shouxieai/infer?style=social"/> : A new tensorrt integrate. Easy to integrate many tasks.

        - [kalfazed/tensorrt_starter](https://github.com/kalfazed/tensorrt_starter) <img src="https://img.shields.io/github/stars/kalfazed/tensorrt_starter?style=social"/> : This repository give a guidline to learn CUDA and TensorRT from the beginning.

        - [hamdiboukamcha/yolov10-tensorrt](https://github.com/hamdiboukamcha/yolov10-tensorrt) <img src="https://img.shields.io/github/stars/hamdiboukamcha/yolov10-tensorrt?style=social"/> : YOLOv10 C++ TensorRT : Real-Time End-to-End Object Detection.

        - [triple-Mu/YOLOv8-TensorRT](https://github.com/triple-Mu/YOLOv8-TensorRT) <img src="https://img.shields.io/github/stars/triple-Mu/YOLOv8-TensorRT?style=social"/> : YOLOv8 using TensorRT accelerate !

        - [FeiYull/TensorRT-Alpha](https://github.com/FeiYull/TensorRT-Alpha) <img src="https://img.shields.io/github/stars/NVIDIA-AI-IOT/torch2trt?style=social"/> : ğŸ”¥ğŸ”¥ğŸ”¥TensorRT for YOLOv8ã€YOLOv8-Poseã€YOLOv8-Segã€YOLOv8-Clsã€YOLOv7ã€YOLOv6ã€YOLOv5ã€YOLONAS......ğŸš€ğŸš€ğŸš€CUDA IS ALL YOU NEED.ğŸğŸğŸ

        - [cyrusbehr/YOLOv8-TensorRT-CPP](https://github.com/cyrusbehr/YOLOv8-TensorRT-CPP) <img src="https://img.shields.io/github/stars/cyrusbehr/YOLOv8-TensorRT-CPP?style=social"/> : YOLOv8 TensorRT C++ Implementation. A C++ Implementation of YoloV8 using TensorRT Supports object detection, semantic segmentation, and body pose estimation.

        - [VIDIA-AI-IOT/torch2trt](https://github.com/NVIDIA-AI-IOT/torch2trt) <img src="https://img.shields.io/github/stars/NVIDIA-AI-IOT/torch2trt?style=social"/> : An easy to use PyTorch to TensorRT converter.

        - [zhiqwang/yolort](https://github.com/zhiqwang/yolort) <img src="https://img.shields.io/github/stars/zhiqwang/yolort?style=social"/> : yolort is a runtime stack for yolov5 on specialized accelerators such as tensorrt, libtorch, onnxruntime, tvm and ncnn. [zhiqwang.com/yolort](https://zhiqwang.com/yolort/)

        - [Linaom1214/TensorRT-For-YOLO-Series](https://github.com/Linaom1214/TensorRT-For-YOLO-Series) <img src="https://img.shields.io/github/stars/Linaom1214/TensorRT-For-YOLO-Series?style=social"/> : YOLO Series TensorRT Python/C++. tensorrt for yolo series (YOLOv8, YOLOv7, YOLOv6....), nms plugin support.

        - [wang-xinyu/tensorrtx](https://github.com/wang-xinyu/tensorrtx) <img src="https://img.shields.io/github/stars/wang-xinyu/tensorrtx?style=social"/> : TensorRTx aims to implement popular deep learning networks with tensorrt network definition APIs.


        - [DefTruth/lite.ai.toolkit](https://github.com/DefTruth/lite.ai.toolkit) <img src="https://img.shields.io/github/stars/DefTruth/lite.ai.toolkit?style=social"/> : ğŸ›  A lite C++ toolkit of awesome AI models with ONNXRuntime, NCNN, MNN and TNN. YOLOX, YOLOP, YOLOv6, YOLOR, MODNet, YOLOX, YOLOv7, YOLOv5. MNN, NCNN, TNN, ONNXRuntime. â€œğŸ› Lite.Ai.ToolKit: ä¸€ä¸ªè½»é‡çº§çš„C++ AIæ¨¡å‹å·¥å…·ç®±ï¼Œç”¨æˆ·å‹å¥½ï¼ˆè¿˜è¡Œå§ï¼‰ï¼Œå¼€ç®±å³ç”¨ã€‚å·²ç»åŒ…æ‹¬ 100+ æµè¡Œçš„å¼€æºæ¨¡å‹ã€‚è¿™æ˜¯ä¸€ä¸ªæ ¹æ®ä¸ªäººå…´è¶£æ•´ç†çš„C++å·¥å…·ç®±ï¼Œ, æ¶µç›–ç›®æ ‡æ£€æµ‹ã€äººè„¸æ£€æµ‹ã€äººè„¸è¯†åˆ«ã€è¯­ä¹‰åˆ†å‰²ã€æŠ å›¾ç­‰é¢†åŸŸã€‚â€

        - [PaddlePaddle/FastDeploy](https://github.com/PaddlePaddle/FastDeploy) <img src="https://img.shields.io/github/stars/PaddlePaddle/FastDeploy?style=social"/> : âš¡ï¸An Easy-to-use and Fast Deep Learning Model Deployment Toolkit for â˜ï¸Cloud ğŸ“±Mobile and ğŸ“¹Edge. Including Image, Video, Text and Audio 20+ main stream scenarios and 150+ SOTA models with end-to-end optimization, multi-platform and multi-framework support.

        - [enazoe/yolo-tensorrt](https://github.com/enazoe/yolo-tensorrt) <img src="https://img.shields.io/github/stars/enazoe/yolo-tensorrt?style=social"/> : TensorRT8.Support Yolov5n,s,m,l,x .darknet -> tensorrt. Yolov4 Yolov3 use raw darknet *.weights and *.cfg fils. If the wrapper is useful to you,please Star it.

        - [guojianyang/cv-detect-robot](https://github.com/guojianyang/cv-detect-robot) <img src="https://img.shields.io/github/stars/guojianyang/cv-detect-robot?style=social"/> : ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥Docker NVIDIA Docker2 YOLOV5 YOLOX YOLO Deepsort TensorRT ROS Deepstream Jetson Nano TX2 NX for High-performance deployment(é«˜æ€§èƒ½éƒ¨ç½²)ã€‚

        - [BlueMirrors/Yolov5-TensorRT](https://github.com/BlueMirrors/Yolov5-TensorRT) <img src="https://img.shields.io/github/stars/BlueMirrors/Yolov5-TensorRT?style=social"/> : Yolov5 TensorRT Implementations.

        - [lewes6369/TensorRT-Yolov3](https://github.com/lewes6369/TensorRT-Yolov3) <img src="https://img.shields.io/github/stars/lewes6369/TensorRT-Yolov3?style=social"/> : TensorRT for Yolov3.

        - [CaoWGG/TensorRT-YOLOv4](https://github.com/CaoWGG/TensorRT-YOLOv4) <img src="https://img.shields.io/github/stars/CaoWGG/TensorRT-YOLOv4?style=social"/> :tensorrt5, yolov4, yolov3,yolov3-tniy,yolov3-tniy-prn.

        - [isarsoft/yolov4-triton-tensorrt](https://github.com/isarsoft/yolov4-triton-tensorrt) <img src="https://img.shields.io/github/stars/isarsoft/yolov4-triton-tensorrt?style=social"/> : YOLOv4 on Triton Inference Server with TensorRT.

        - [TrojanXu/yolov5-tensorrt](https://github.com/TrojanXu/yolov5-tensorrt) <img src="https://img.shields.io/github/stars/TrojanXu/yolov5-tensorrt?style=social"/> : A tensorrt implementation of yolov5.

        - [tjuskyzhang/Scaled-YOLOv4-TensorRT](https://github.com/tjuskyzhang/Scaled-YOLOv4-TensorRT) <img src="https://img.shields.io/github/stars/tjuskyzhang/Scaled-YOLOv4-TensorRT?style=social"/> : Implement yolov4-tiny-tensorrt, yolov4-csp-tensorrt, yolov4-large-tensorrt(p5, p6, p7) layer by layer using TensorRT API.

        - [Syencil/tensorRT](https://github.com/Syencil/tensorRT) <img src="https://img.shields.io/github/stars/Syencil/tensorRT?style=social"/> : TensorRT-7 Network Lib åŒ…æ‹¬å¸¸ç”¨ç›®æ ‡æ£€æµ‹ã€å…³é”®ç‚¹æ£€æµ‹ã€äººè„¸æ£€æµ‹ã€OCRç­‰ å¯è®­ç»ƒè‡ªå·±æ•°æ®ã€‚

        - [SeanAvery/yolov5-tensorrt](https://github.com/SeanAvery/yolov5-tensorrt) <img src="https://img.shields.io/github/stars/SeanAvery/yolov5-tensorrt?style=social"/> : YOLOv5 in TensorRT.

        - [Monday-Leo/YOLOv7_Tensorrt](https://github.com/Monday-Leo/YOLOv7_Tensorrt) <img src="https://img.shields.io/github/stars/Monday-Leo/YOLOv7_Tensorrt?style=social"/> : A simple implementation of Tensorrt YOLOv7.

        - [ibaiGorordo/ONNX-YOLOv6-Object-Detection](https://github.com/ibaiGorordo/ONNX-YOLOv6-Object-Detection) <img src="https://img.shields.io/github/stars/ibaiGorordo/ONNX-YOLOv6-Object-Detection?style=social"/> : Python scripts performing object detection using the YOLOv6 model in ONNX.

        - [ibaiGorordo/ONNX-YOLOv7-Object-Detection](https://github.com/ibaiGorordo/ONNX-YOLOv7-Object-Detection) <img src="https://img.shields.io/github/stars/ibaiGorordo/ONNX-YOLOv7-Object-Detection?style=social"/> : Python scripts performing object detection using the YOLOv7 model in ONNX.

        - [triple-Mu/yolov7](https://github.com/triple-Mu/yolov7) <img src="https://img.shields.io/github/stars/triple-Mu/yolov7?style=social"/> : End2end TensorRT YOLOv7.

        - [hewen0901/yolov7_trt](https://github.com/hewen0901/yolov7_trt) <img src="https://img.shields.io/github/stars/hewen0901/yolov7_trt?style=social"/> : yolov7ç›®æ ‡æ£€æµ‹ç®—æ³•çš„c++ tensorrtéƒ¨ç½²ä»£ç ã€‚

        - [tsutof/tiny_yolov2_onnx_cam](https://github.com/tsutof/tiny_yolov2_onnx_cam) <img src="https://img.shields.io/github/stars/tsutof/tiny_yolov2_onnx_cam?style=social"/> : Tiny YOLO v2 Inference Application with NVIDIA TensorRT.

        - [Monday-Leo/Yolov5_Tensorrt_Win10](https://github.com/Monday-Leo/Yolov5_Tensorrt_Win10) <img src="https://img.shields.io/github/stars/Monday-Leo/Yolov5_Tensorrt_Win10?style=social"/> : A simple implementation of tensorrt yolov5 python/c++ğŸ”¥

        - [Wulingtian/yolov5_tensorrt_int8](https://github.com/Wulingtian/yolov5_tensorrt_int8) <img src="https://img.shields.io/github/stars/Wulingtian/yolov5_tensorrt_int8?style=social"/> : TensorRT int8 é‡åŒ–éƒ¨ç½² yolov5s æ¨¡å‹ï¼Œå®æµ‹3.3msä¸€å¸§ï¼

        - [Wulingtian/yolov5_tensorrt_int8_tools](https://github.com/Wulingtian/yolov5_tensorrt_int8_tools) <img src="https://img.shields.io/github/stars/Wulingtian/yolov5_tensorrt_int8_tools?style=social"/> : tensorrt int8 é‡åŒ–yolov5 onnxæ¨¡å‹ã€‚

        - [MadaoFY/yolov5_TensorRT_inference](https://github.com/MadaoFY/yolov5_TensorRT_inference) <img src="https://img.shields.io/github/stars/MadaoFY/yolov5_TensorRT_inference?style=social"/> : è®°å½•yolov5çš„TensorRTé‡åŒ–åŠæ¨ç†ä»£ç ï¼Œç»å®æµ‹å¯è¿è¡ŒäºJetsonå¹³å°ã€‚

        - [ibaiGorordo/ONNX-YOLOv8-Object-Detection](https://github.com/ibaiGorordo/ONNX-YOLOv8-Object-Detection) <img src="https://img.shields.io/github/stars/ibaiGorordo/ONNX-YOLOv8-Object-Detection?style=social"/> : Python scripts performing object detection using the YOLOv8 model in ONNX.

        - [we0091234/yolov8-tensorrt](https://github.com/we0091234/yolov8-tensorrt) <img src="https://img.shields.io/github/stars/we0091234/yolov8-tensorrt?style=social"/> : yolov8 tensorrt åŠ é€Ÿ.

        - [FeiYull/yolov8-tensorrt](https://github.com/FeiYull/yolov8-tensorrt) <img src="https://img.shields.io/github/stars/FeiYull/yolov8-tensorrt?style=social"/> : YOLOv8çš„TensorRT+CUDAåŠ é€Ÿéƒ¨ç½²ï¼Œä»£ç å¯åœ¨Winã€Linuxä¸‹è¿è¡Œã€‚

        - [cvdong/YOLO_TRT_SIM](https://github.com/cvdong/YOLO_TRT_SIM) <img src="https://img.shields.io/github/stars/cvdong/YOLO_TRT_SIM?style=social"/> : ğŸ‡ ä¸€å¥—ä»£ç åŒæ—¶æ”¯æŒYOLO X, V5, V6, V7, V8 TRTæ¨ç† â„¢ï¸ ğŸ” ,å‰åå¤„ç†å‡ç”±CUDAæ ¸å‡½æ•°å®ç° CPP/CUDAğŸš€

        - [cvdong/YOLO_TRT_PY](https://github.com/cvdong/YOLO_TRT_PY) <img src="https://img.shields.io/github/stars/cvdong/YOLO_TRT_PY?style=social"/> : ğŸ° ä¸€å¥—ä»£ç åŒæ—¶æ”¯æŒYOLOV5, V6, V7, V8 TRTæ¨ç† â„¢ï¸ PYTHON âœˆï¸

        - [Psynosaur/Jetson-SecVision](https://github.com/Psynosaur/Jetson-SecVision) <img src="https://img.shields.io/github/stars/Psynosaur/Jetson-SecVision?style=social"/> : Person detection for Hikvision DVR with AlarmIO ports, uses TensorRT and yolov4.

        - [tatsuya-fukuoka/yolov7-onnx-infer](https://github.com/tatsuya-fukuoka/yolov7-onnx-infer) <img src="https://img.shields.io/github/stars/tatsuya-fukuoka/yolov7-onnx-infer?style=social"/> : Inference with yolov7's onnx model.

        - [MadaoFY/yolov5_TensorRT_inference](https://github.com/MadaoFY/yolov5_TensorRT_inference) <img src="https://img.shields.io/github/stars/MadaoFY/yolov5_TensorRT_inference?style=social"/> : è®°å½•yolov5çš„TensorRTé‡åŒ–åŠæ¨ç†ä»£ç ï¼Œç»å®æµ‹å¯è¿è¡ŒäºJetsonå¹³å°ã€‚

        - [ervgan/yolov5_tensorrt_inference](https://github.com/ervgan/yolov5_tensorrt_inference) <img src="https://img.shields.io/github/stars/ervgan/yolov5_tensorrt_inference?style=social"/> : TensorRT cpp inference for Yolov5 model. Supports yolov5 v1.0, v2.0, v3.0, v3.1, v4.0, v5.0, v6.0, v6.2, v7.0.

        - [AlbinZhu/easy-trt](https://github.com/AlbinZhu/easy-trt) <img src="https://img.shields.io/github/stars/AlbinZhu/easy-trt?style=social"/> : TensorRT for YOLOv10 with CUDA.


    - #### Signal Processing

        - [Alisah-Ozcan/GPU-FFT](https://github.com/Alisah-Ozcan/GPU-FFT) <img src="https://img.shields.io/github/stars/Alisah-Ozcan/GPU-FFT?style=social"/> : Welcome to the GPU-FFT-Optimization repository! We present cutting-edge algorithms and implementations for optimizing the Fast Fourier Transform (FFT) on Graphics Processing Units (GPUs).

    - #### Mesh Processing

        - [owensgroup/RXMesh](https://github.com/owensgroup/RXMesh) <img src="https://img.shields.io/github/stars/owensgroup/RXMesh?style=social"/> : GPU-accelerated triangle mesh processing. RXMesh is a surface triangle mesh data structure and programming model for processing static meshes on the GPU. RXMesh aims at provides a high-performance, generic, and compact data structure that can handle meshes regardless of their quality (e.g., non-manifold).

    - #### Graph Analytics

        - [rapidsai/cugraphs](https://github.com/rapidsai/cugraph) <img src="https://img.shields.io/github/stars/rapidsai/cugraph?style=social"/> : cuGraph - RAPIDS Graph Analytics Library. [docs.rapids.ai/api/cugraph/stable/](https://docs.rapids.ai/api/cugraph/stable/)





## Blogs

  - ### CUDA and TensorRT Blogs

    - å¾®ä¿¡å…¬ä¼—å·ã€ŒNVIDIAè‹±ä¼Ÿè¾¾ã€
        - [2023-10-27ï¼Œç°å·²å…¬å¼€å‘å¸ƒï¼æ¬¢è¿ä½¿ç”¨ NVIDIA TensorRT-LLM ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹æ¨ç†](https://mp.weixin.qq.com/s/QaSbvyAmI6XXtr0y6W4LNQ)
        - [2023-11-24ï¼Œä½¿ç”¨ NVIDIA IGX Orin å¼€å‘è€…å¥—ä»¶åœ¨è¾¹ç¼˜éƒ¨ç½²å¤§è¯­è¨€æ¨¡å‹](https://mp.weixin.qq.com/s/TOTVc5ntQJfH-DJ4_8uNTQ)
        - [2024-06-03ï¼ŒCOMPUTEX 2024 | â€œåŠ é€Ÿä¸€åˆ‡â€ï¼ŒNVIDIA CEO é»„ä»å‹‹åœ¨ COMPUTEX å¼€å¹•å‰å‘è¡¨ä¸»é¢˜æ¼”è®²](https://mp.weixin.qq.com/s/usHo79-ssQiX0Rt5dvJ-sQ)
        - [2024-06-19ï¼ŒNVIDIA CEO é»„ä»å‹‹å¯„è¯­æ¯•ä¸šç”Ÿï¼šâ€œå¯¹éå¸¸è§„ã€æœªç»æ¢ç´¢çš„ä¸œè¥¿ä¿æŒä¿¡ä»°â€](https://mp.weixin.qq.com/s/L8Lv6pz9BIgzLdm6qZm6dQ)
    - å¾®ä¿¡å…¬ä¼—å·ã€ŒNVIDIAè‹±ä¼Ÿè¾¾ä¼ä¸šè§£å†³æ–¹æ¡ˆã€
        - [2024-04-24ï¼ŒNVIDIA GPU æ¶æ„ä¸‹çš„ FP8 è®­ç»ƒä¸æ¨ç†](https://mp.weixin.qq.com/s/KV4XC9WT-8mfpmEzflIuvw)
        - [2024-06-14ï¼Œåˆåˆ›åŠ é€Ÿè®¡åˆ’ | åŸºäº NVIDIA Jetson å¹³å°ï¼Œå›½è®¯èŠ¯å¾®å®ç°å¤§å°è„‘ç«¯åˆ°ç«¯ååŒæ§åˆ¶](https://mp.weixin.qq.com/s/R7U5JUgUCMK4rvtIpgStKQ)
        - [2024-06-20ï¼ŒNVIDIA Isaac Sim 4.0 å’Œ NVIDIA Isaac Lab ä¸ºæœºå™¨äººå·¥ä½œæµå’Œä»¿çœŸæä¾›å¼ºå¤§åŠ©åŠ›](https://mp.weixin.qq.com/s/BYqLDexhHnPMVsQMPLWpOA)
        - [2024-06-21ï¼Œæ¶ˆé™¤ä»¿çœŸä¸ç°å®ä¹‹é—´çš„å·®è·ï¼šä½¿ç”¨ NVIDIA Isaac Lab è®­ç»ƒ Spot å››è¶³æœºå™¨äººè¿åŠ¨](https://mp.weixin.qq.com/s/Nb4oMxijBofiidSAHkafag)
        - [2024-07-01ï¼ŒNVIDIA ç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆåŠ©åŠ›ç†æƒ³æ±½è½¦æ‰“é€ æ™ºèƒ½é©¾é©¶ä½“éªŒä¸ä¸ªæ€§åŒ–è½¦å†…ç©ºé—´](https://mp.weixin.qq.com/s/gmkYFj5BcJZHO4GJ_b8pyQ)
        - [2024-11-27ï¼ŒNVIDIA TensorRT-LLM Roadmap ç°å·²åœ¨ GitHub ä¸Šå…¬å¼€å‘å¸ƒï¼](https://mp.weixin.qq.com/s/zqAkxmWinwNMbcIBVA1hnA)
    - å¾®ä¿¡å…¬ä¼—å·ã€ŒAIä¸æ­¢ç®—æ³•ã€
        - [2024-03-20ï¼ŒC++æ¨¡æ¿æ¨å¯¼å†ç‚«æŠ€ï¼šç»Ÿä¸€AIå„ä¸ªdeviceå„ä¸ªkernelçš„è°ƒç”¨å’Œåˆ†å‘](https://mp.weixin.qq.com/s/r1XFocdVrfuArDWzpBYdAg)
        - [2024-04-09ï¼Œå…¨ç½‘é¦–ç¯‡ä»tensorRT-LLM MoE CUDA kernelè§’åº¦ç†è§£Mixtral-8x7bçš„æ¨ç†åŠ é€ŸåŠå±•æœ›](https://mp.weixin.qq.com/s/3PsVUba-kTLIHK_s0RA2ow)
        - [2024-05-10ï¼Œå…¨é¢æ¢ç©¶GPU SMå†…CUDA core-Tensor coreèƒ½å¦åŒæ—¶è®¡ç®—ï¼Ÿ(ä¸Šç¯‡)](https://mp.weixin.qq.com/s/YASkRa12Ecr6fLtupP1WHg)
        - [2024-05-16ï¼Œå…¨é¢æ¢ç©¶GPU SMå†…CUDA core-Tensor coreèƒ½å¦åŒæ—¶è®¡ç®—ï¼Ÿ(ä¸‹ç¯‡)](https://mp.weixin.qq.com/s/Jcu_HkAMiMXYagBjNhSCZQ)
        - [2024-10-09ï¼Œæ·±å…¥è§£è¯»tensorRT-LLMçš„å…³é”®æŠ€æœ¯ (æœªå®Œå¾…ç»­)](https://mp.weixin.qq.com/s/2l5Ko2Q-iNOL3PpwpUdArw)
        - [2025-01-20ï¼Œæ·±å…¥è§£æcutlassçš„è¯ç”Ÿå†ç¨‹ã€ç‰¹æ€§å’Œå¯¹å‹å•†çš„å„å¤§ä¼˜åŠ¿](https://mp.weixin.qq.com/s/-dHq2DOzsEiJQe1LvsRxrQ)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œæ¾å³°ç§‘æŠ€PerfXLabã€
        - [2022-10-18ï¼Œæ·±å…¥æµ…å‡ºGPUä¼˜åŒ–ç³»åˆ—ï¼šreduceä¼˜åŒ–](https://mp.weixin.qq.com/s/tNDRd18Ol56U-spoinzttg)
        - [2022-10-31ï¼Œæ·±å…¥æµ…å‡ºGPUä¼˜åŒ–ç³»åˆ—ï¼šspmvä¼˜åŒ–](https://mp.weixin.qq.com/s/JIqUbPFtYc3fs_cvKi1r3A)
        - [2023-05-24ï¼Œæ·±å…¥æµ…å‡ºGPUä¼˜åŒ–ç³»åˆ—ï¼šgemvä¼˜åŒ–](https://mp.weixin.qq.com/s/VCuJMrwGwyf9QCaaXcKmAg)
        - [2023-05-24ï¼Œæ·±å…¥æµ…å‡ºGPUä¼˜åŒ–ç³»åˆ—ï¼šGEMMä¼˜åŒ–ï¼ˆä¸€ï¼‰](https://mp.weixin.qq.com/s/4aPW_93IV54lzs5JRn0JiA)
        - [2023-06-02ï¼Œæ·±å…¥æµ…å‡ºGPUä¼˜åŒ–ç³»åˆ—ï¼šGEMMä¼˜åŒ–ï¼ˆäºŒï¼‰](https://mp.weixin.qq.com/s/1q5ocZ7vDDsvew3HNo_9Vg)
        - [2023-06-16ï¼Œæ·±å…¥æµ…å‡ºGPUä¼˜åŒ–ç³»åˆ—ï¼šGEMMä¼˜åŒ–ï¼ˆä¸‰ï¼‰](https://mp.weixin.qq.com/s/13Nw6fubNLOMFR3ROc0z0w)
        - [2023-06-26ï¼Œæ·±å…¥æµ…å‡ºGPUä¼˜åŒ–ç³»åˆ—ï¼šelementwiseä¼˜åŒ–åŠCUDAå·¥å…·é“¾ä»‹ç»](https://mp.weixin.qq.com/s/5h0lpKun0DlbefH_y-AdMg)
        - [2023-06-27ï¼Œæ¼«è°ˆé«˜æ€§èƒ½è®¡ç®—ä¸æ€§èƒ½ä¼˜åŒ–ï¼šè®¿å­˜](https://mp.weixin.qq.com/s/9BhJOqkbNbwXcSJuHUU52w)
        - [2024-07-04ï¼Œæ¾å³°ç§‘æŠ€ç ”å‘çš„é«˜æ€§èƒ½è®¡ç®—åŸè¯­åº“PerfIPPåº“æŠ€æœ¯ç™½çš®ä¹¦å‘å¸ƒï¼ˆé™„ä¸‹è½½ï¼‰](https://mp.weixin.qq.com/s/Hd8S7bJGjvz9GUK6Q0lWSw)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œå¤§çŒ¿æ¬ç –ç®€è®°ã€
        - [2024-03-11ï¼Œå›¾è§£Mixtral 8 * 7bæ¨ç†ä¼˜åŒ–åŸç†ä¸æºç å®ç°](https://mp.weixin.qq.com/s/jjZQ4A-rvk_e-woKLlNTVQ)
        - [2024-03-29ï¼Œå›¾è§£å¤§æ¨¡å‹è®¡ç®—åŠ é€Ÿç³»åˆ—ä¹‹ï¼švLLMæ ¸å¿ƒæŠ€æœ¯PagedAttentionåŸç†](https://mp.weixin.qq.com/s/-5EniAmFf1v9RdxI5-CwiQ)
        - [2024-04-06ï¼Œå›¾è§£å¤§æ¨¡å‹è®¡ç®—åŠ é€Ÿç³»åˆ—ï¼švLLMæºç è§£æ1ï¼Œæ•´ä½“æ¶æ„](https://mp.weixin.qq.com/s/r_t6_zMvPT7za82MZX4oRA)
        - [2024-04-12ï¼Œå›¾è§£å¤§æ¨¡å‹è®¡ç®—åŠ é€Ÿç³»åˆ—ï¼švLLMæºç è§£æ2ï¼Œè°ƒåº¦å™¨ç­–ç•¥(Scheduler)](https://mp.weixin.qq.com/s/UCdqQUM_9a36uXkO36wpSg)
        - [2024-04-19ï¼Œä»å•¥ä¹Ÿä¸ä¼šåˆ°Cuda GEMMä¼˜åŒ–](https://mp.weixin.qq.com/s/YLrsu1KAhzG8gFQ2L-TaMA)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œoldpanåšå®¢ã€
        - [2024-03-19ï¼ŒNVIDIAå¤§è¯­è¨€æ¨¡å‹è½åœ°çš„å…¨æµç¨‹è§£æ](https://mp.weixin.qq.com/s/-sNnuDvkucUB_9K9RBfDEw)
        - [2024-03-20ï¼ŒTensorRT-LLMåˆæ¢ï¼ˆäºŒï¼‰ç®€æäº†ç»“æ„ï¼Œç”¨çš„æ›´æ˜ç™½](https://mp.weixin.qq.com/s/Jk-AK84sllBbkDDpvkv62w)
        - [2024-03-21ï¼Œé«˜æ€§èƒ½ LLM æ¨ç†æ¡†æ¶çš„è®¾è®¡ä¸å®ç°](https://mp.weixin.qq.com/s/zys9KvQWbbdRHkOyhzZqUw)
        - [2024-04-15ï¼Œ[æ·±å…¥åˆ†æCUTLASSç³»åˆ—] 0x01 cutlass æºç åˆ†æ(é›¶) --- è½¯ä»¶æ¶æ„(é™„ncuæ€§èƒ½åˆ†ææ–¹æ³•)](https://mp.weixin.qq.com/s/sLvZoWgILuRnvyiimMgeaQ)
        - [2024-04-21ï¼Œææ‡‚ NVIDIA GPU æ€§èƒ½æŒ‡æ ‡ å¾ˆå®¹æ˜“å¼„æ··çš„ä¸€ä¸ªæ¦‚å¿µï¼š Utilization vs Saturation](https://mp.weixin.qq.com/s/6PcF2RwGdm1G0JllGSS3jw)
        - [2024-04-22ï¼Œå¿«é€Ÿæå‡æ€§èƒ½ï¼Œå¦‚ä½•æ›´å¥½åœ°ä½¿ç”¨GPUï¼ˆä¸Šï¼‰](https://mp.weixin.qq.com/s/dUj058iBzYm-J2vlS5DfNA)
        - [2024-05-14ï¼Œå¿«é€Ÿæå‡æ€§èƒ½ï¼Œå¦‚ä½•æ›´å¥½åœ°ä½¿ç”¨GPUï¼ˆä¸‹ï¼‰](https://mp.weixin.qq.com/s/NPcCHlLjBZeUiAhQOHX5qA)
        - [2024-05-22ï¼Œå¤§æ¨¡å‹ç²¾åº¦ï¼ˆFP16ï¼ŒFP32ï¼ŒBF16ï¼‰è¯¦è§£ä¸å®è·µ](https://mp.weixin.qq.com/s/95CUl1bGN-fSvmAbH0O-DA)
        - [2024-07-24ï¼ŒCUDAæ€§èƒ½ç®€æ˜“ä¼˜åŒ–ï¼ˆä¸€ï¼‰èƒŒæ™¯çŸ¥è¯†](https://mp.weixin.qq.com/s/mFMlBh3zPZaCRWQH-neeDA)
        - [2024-08-06ï¼Œå¦‚ä½•æŠŠ PyTorch çš„ GPU åˆ©ç”¨ç‡æå‡åˆ° 100% ?](https://mp.weixin.qq.com/s/Hwc32FDTOMaZSbbtNInCtg)
        - [2024-08-13ï¼ŒTensorRT-LLMåˆæ¢ï¼ˆä¸‰ï¼‰æœ€ä½³éƒ¨ç½²å®è·µ](https://mp.weixin.qq.com/s/BiSvYW0-Nb6qf-bTXTRUwg)
    - å¾®ä¿¡å…¬ä¼—å·ã€ŒDeepPromptingã€
        - [2024-01-09ï¼ŒLLMæ¨ç†åº“TensorRT-LLMæ·±å…¥åˆ†æ](https://mp.weixin.qq.com/s/hI6maWtVGHnTi0uGPj6tmA)
        - [2024-04-10ï¼Œä¸€æ–‡ä¸Šæ‰‹ Tensor CoreæŒ‡ä»¤çº§ç¼–ç¨‹](https://mp.weixin.qq.com/s/Gi8ExdfErUkfWu3oRyKvBw)
        - [2024-04-23ï¼Œå¤§è¯­è¨€æ¨¡å‹é‡åŒ–](https://mp.weixin.qq.com/s/3RUVgfrLdxyeoWX1R2Hq-Q)
        - [2024-04-25ï¼ŒåŠ¨æ‰‹å®ç°æ··åˆç²¾åº¦çŸ©é˜µä¹˜CUDAå†…æ ¸](https://mp.weixin.qq.com/s/JGYFOsPvUSNMQWjR1gKOOg)
        - [2024-04-26ï¼Œä¸€æ–‡äº†è§£CUDAçŸ©é˜µä¹˜ç¼–ç¨‹](https://mp.weixin.qq.com/s/vG7d7-tAt-mXOgRSb-jZRA)
    - å¾®ä¿¡å…¬ä¼—å·ã€ŒGiantPandaCVã€
        - [2024-04-20ï¼ŒTensor Cores ä½¿ç”¨ä»‹ç»](https://mp.weixin.qq.com/s/Mr-yR_YW5nNKV2dSrr5U2Q)
        - [2024-05-27ï¼Œ[å¹¶è¡Œè®­ç»ƒ]Context Parallelismçš„åŸç†ä¸ä»£ç æµ…æ](https://mp.weixin.qq.com/s/vXWUUtAQNkBUpgDIJV8C0w)
        - [2024-06-20ï¼Œ FP8é‡åŒ–è§£è¯»--8bitä¸‹æœ€ä¼˜æ–¹æ¡ˆï¼Ÿï¼ˆä¸€ï¼‰](https://mp.weixin.qq.com/s/WcFG7mmsEwrL0g3dSJTC5A)
        - [2024-07-01ï¼ŒCUDA-MODE è¯¾ç¨‹ç¬”è®° ç¬¬ä¸€è¯¾: å¦‚ä½•åœ¨ PyTorch ä¸­ profile CUDA kernels](https://mp.weixin.qq.com/s/owF7AFR61SLrOosUPdZPQQ)
        - [2024-07-04ï¼ŒCUDA-MODE ç¬¬ä¸€è¯¾è¯¾åå®æˆ˜ï¼ˆä¸Šï¼‰](https://mp.weixin.qq.com/s/9XeJPWUsKTaMU2OdPkL-OQ)
        - [2024-07-06ï¼ŒCUDA-MODE è¯¾ç¨‹ç¬”è®° ç¬¬äºŒè¯¾: PMPP ä¹¦çš„ç¬¬1-3ç« é€Ÿé€š](https://mp.weixin.qq.com/s/y0fYn8gUqHqEoRO41ftKnA)
        - [2024-07-13ï¼ŒCUDA-MODE è¯¾ç¨‹ç¬”è®° ç¬¬å››è¯¾: PMPP ä¹¦çš„ç¬¬4-5ç« ç¬”è®°](https://mp.weixin.qq.com/s/P87c8LRJ1CEOOyaQw8L-cA)
        - [2024-07-18ï¼ŒCUDA-MODEè¯¾ç¨‹ç¬”è®° ç¬¬6è¯¾: å¦‚ä½•ä¼˜åŒ–PyTorchä¸­çš„ä¼˜åŒ–å™¨](https://mp.weixin.qq.com/s/qxPYdGZ71DKVLnnYxmvUVA)
        - [2024-07-19ï¼ŒCUDA-MODE ç¬¬ä¸€è¯¾è¯¾åå®æˆ˜ï¼ˆä¸‹ï¼‰](https://mp.weixin.qq.com/s/FCqnQESCQTtlqCG_BSLulA)
        - [2024-07-23ï¼ŒCUTLASS 2.x & CUTLASS 3.x Intro å­¦ä¹ ç¬”è®°](https://mp.weixin.qq.com/s/r9b1dGyOr82ooMl4LD1n_Q)
        - [2024-07-28ï¼ŒCUDA-MODEè¯¾ç¨‹ç¬”è®° ç¬¬7è¯¾: Quantization Cuda vs Triton](https://mp.weixin.qq.com/s/1gCgpp49NF7sDw__EpO-nw)
        - [2024-08-01ï¼ŒTRT-LLMä¸­çš„Quantization GEMMï¼ˆAmpere Mixed GEMMï¼‰CUTLASS 2.x è¯¾ç¨‹å­¦ä¹ ç¬”è®°](https://mp.weixin.qq.com/s/NPytrkchX25YRBc_6Zy6nA)
        - [2024-08-05ï¼ŒCUDA-MODEè¯¾ç¨‹ç¬”è®° ç¬¬8è¯¾: CUDAæ€§èƒ½æ£€æŸ¥æ¸…å•](https://mp.weixin.qq.com/s/zJLDVF-yjuZ_lMjaCHoS5g)
        - [2024-09-12ï¼ŒCUDA-MODEè¯¾ç¨‹ç¬”è®° ç¬¬12è¯¾ï¼ŒFlash Attention](hhttps://mp.weixin.qq.com/s/IBeBHO5WlS5BfyL0nZaDHg)
    - å¾®ä¿¡å…¬ä¼—å·ã€ŒGPUSå¼€å‘è€…ã€
        - [2023-10-30ï¼Œåˆ©ç”¨NVIDIA Jetson Orinçš„å¼ºå¤§èƒ½åŠ›æ‰§è¡Œæœ¬åœ°LLMæ¨¡å‹](https://mp.weixin.qq.com/s/6J7fEnumqpzSGrG3plcInw)
        - [2024-05-07ï¼ŒåŸºäºNVIDIA Jetson AGX Orinå’ŒAudio2Faceåšä¸€ä¸ªAIèŠå¤©æ•°å­—äºº](https://mp.weixin.qq.com/s/7z0uU58IxwoXcI4bZ3z68g)
        - [2024-05-14ï¼ŒCUDAä¸OpenCLï¼šå¹¶è¡Œè®¡ç®—é©å‘½çš„å†²çªä¸æœªæ¥](https://mp.weixin.qq.com/s/h0nBvuV8nnfsbX1mjXAXVw)
        - [2024-10-08ï¼ŒNVIDIA Jetsonå¹³å°åŠ©åŠ›Instacartï¼Œå®ç°è¶…å¸‚æ™ºèƒ½è´­ç‰©æ— ç¼ä½“éªŒ](https://mp.weixin.qq.com/s/Q9x83ts0boNoQHZhSIpgUw)
        - [2024-11-28ï¼ŒTensorRT-LLMï¼šå¼€å¯Jetsonå¹³å°ä¸Šå¤§è¯­è¨€æ¨¡å‹æ¨ç†çš„æ–°ç¯‡ç« ](https://mp.weixin.qq.com/s/FGg1s__LORE6SOzI_VsJYw)
        - [2024-11-29ï¼Œåœ¨ Nvidia Jetson AGX Orin ä¸Šä½¿ç”¨ TensorRT-LLM è¿è¡Œ LLM](https://mp.weixin.qq.com/s/PhrusT3NisyhxtsO7G5U1g)
        - [2025-04-07ï¼ŒCUDAç¼–ç¨‹é©å‘½ï¼šcuTileåˆ†å—è®¡ç®—èµ‹èƒ½é«˜æ•ˆGPUå¼€å‘](https://mp.weixin.qq.com/s/YfxJkAzr_5FRuY828Pv9kg)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œæœºå™¨å­¦ä¹ ç ”ç©¶ç»„è®¢é˜…ã€
        - [2017-12-07ï¼Œã€æ¨èã€‘CUTLASSï¼šCUDA C++é«˜æ€§èƒ½çº¿æ€§ä»£æ•°è¿ç®—åº“](https://mp.weixin.qq.com/s/EDmbQ4y3nnkYiHhl3HG_HA)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œè‡ªåŠ¨é©¾é©¶ä¹‹å¿ƒã€
        - [2024-02-28ï¼Œç†¬äº†å‡ ä¸ªé€šå®µï¼Œæˆ‘å†™äº†ä»½CUDAæ–°æ‰‹å…¥é—¨ä»£ç ](https://mp.weixin.qq.com/s/UXIzQ9SYhtN4q8VfzNXDqA)
        - [2024-03-24ï¼ŒCUDAä¹‹é€šç”¨çŸ©é˜µä¹˜æ³•ï¼šä»å…¥é—¨åˆ°ç†Ÿç»ƒï¼](https://mp.weixin.qq.com/s/n9c-SZl5I_oj4N2l3I5kRg)
        - [2024-05-13ï¼ŒShared memoryï¼CUDAæ•°æ®æ‹·è´é€Ÿåº¦æ‹‰æ»¡~](https://mp.weixin.qq.com/s/P5CdO3QCSQKuj3nWjS_2yA)
    - å¾®ä¿¡å…¬ä¼—å·ã€ŒMeet DSAã€
        - [2024-03-29ï¼Œå¤§è¯­è¨€æ¨¡å‹ç¡¬ä»¶åŠ é€Ÿå™¨ç»¼è¿°](https://mp.weixin.qq.com/s/rtq8e_zVUWLc-vkT4V0qzQ)
    - å¾®ä¿¡å…¬ä¼—å·ã€ŒAIå¯’æ­¦çºªã€
        - [2024-04-10ï¼Œã€å¤ªç–¯ç‹‚äº†ã€‘ç”¨ 1000 è¡Œçº¯ C ä»£ç å®ç° GPT-2 è®­ç»ƒï¼šAndrej Karpathyé‡å¡‘LLMè®­ç»ƒæ ¼å±€](https://mp.weixin.qq.com/s/hNKWVqepbega6YPf48b8ag)
        - [2024-04-14ï¼Œã€å…¨çƒé»‘å®¢åŠ æŒã€‘Karpathy 1000è¡Œçº¯Cè®­ç»ƒå¤§æ¨¡å‹é€Ÿåº¦å·²è¿½å¹³PyTorch](https://mp.weixin.qq.com/s/VvwDhMmq80yN-Wcb8s3aiQ)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œå…³äºNLPé‚£äº›ä½ ä¸çŸ¥é“çš„äº‹ã€
        - [2024-01-26ï¼ŒåŸºäºTensorRT-LLMçš„å¤§æ¨¡å‹éƒ¨ç½²(é€Ÿé€šç¬”è®°)](https://mp.weixin.qq.com/s/2d6ihFFDTDfppYbjtBPHMw)
    - å¾®ä¿¡å…¬ä¼—å·ã€ŒInfoQã€
        - [2024-04-09ï¼Œâ€œçœŸç”·äººå°±åº”è¯¥ç”¨ C ç¼–ç¨‹â€ï¼ç”¨ 1000 è¡Œ C ä»£ç æ‰‹æ“äº†ä¸€ä¸ªå¤§æ¨¡å‹ï¼ŒMac å³å¯è¿è¡Œï¼Œç‰¹æ–¯æ‹‰å‰AIæ€»ç›‘çˆ†ç«ç§‘æ™® LLM](https://mp.weixin.qq.com/s/qb0dhdFnXZS4LeW2mvG6fg)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œæœºå™¨ä¹‹å¿ƒã€
        - [2024-04-09ï¼Œçº¯Cè¯­è¨€æ‰‹æ“GPT-2ï¼Œå‰OpenAIã€ç‰¹æ–¯æ‹‰é«˜ç®¡æ–°é¡¹ç›®ç«äº†](https://mp.weixin.qq.com/s/YMuq9Jo9Nibl1QFbLNxazg)
        - [2024-05-20ï¼Œé¦–ä¸ªGPUé«˜çº§è¯­è¨€ï¼Œå¤§è§„æ¨¡å¹¶è¡Œå°±åƒå†™Pythonï¼Œå·²è·8500 Star](https://mp.weixin.qq.com/s/dC7Z5Rk05sM7ND7bYUsrZA)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œæ–°æ™ºå…ƒã€
        - [2023-09-10ï¼ŒH100æ¨ç†é£™å‡8å€ï¼è‹±ä¼Ÿè¾¾å®˜å®£å¼€æºTensorRT-LLMï¼Œæ”¯æŒ10+æ¨¡å‹](https://mp.weixin.qq.com/s/xcNQBG69XkS6mOstzqROAw)
        - [2024-04-07ï¼ŒLlamaæé€Ÿ500%ï¼è°·æ­Œç¾å¥³ç¨‹åºå‘˜æ‰‹æ“çŸ©é˜µä¹˜æ³•å†…æ ¸](https://mp.weixin.qq.com/s/2ROw_Tmmh4NHf8WOiwnJLg)
        - [2024-04-09ï¼Œ1000è¡ŒCè¯­è¨€æ“å‡ºGPT-2ï¼AIå¤§ç¥Karpathyæ–°é¡¹ç›®åˆšä¸Šçº¿å°±ç‹‚æ½2.5kæ˜Ÿ](https://mp.weixin.qq.com/s/_W2GlbO8nAfpLPtRtQJ-yw)
    - å¾®ä¿¡å…¬ä¼—å·ã€ŒGitHubStoreã€
        - [2024-04-11ï¼Œllm.cï¼šå®ç°äº†å¤§è¯­è¨€æ¨¡å‹(LLM)è®­ç»ƒçš„ç®€å•ã€çº¯ C/CUDA ç‰ˆæœ¬ï¼Œæ— éœ€ PyTorch æˆ– cPython](https://mp.weixin.qq.com/s/7cHYDBHqs8ClkijI-Fya9A)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œäº‘äº‘ä¼—ç”Ÿsã€
        - [2024-04-17ï¼ŒNVIDIAå¸Œæœ›æœ‰æ›´å¤šæ”¯æŒCUDAçš„ç¼–ç¨‹è¯­è¨€](https://mp.weixin.qq.com/s/jABUruiJwjhGstbPG3U2Fw)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œæ‰‹å†™AIã€
        - [2022-10-16ï¼ŒTensorRT/CUDAè¶…å…¨ä»£ç èµ„æ–™ä»“åº“](https://mp.weixin.qq.com/s/WXZXVlAohZn2YJ490pddpQ)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œç¾å›¢æŠ€æœ¯å›¢é˜Ÿã€
        - [2024-04-11ï¼Œç¾å›¢å¤–å–åŸºäºGPUçš„å‘é‡æ£€ç´¢ç³»ç»Ÿå®è·µ](https://mp.weixin.qq.com/s/pPl-anyQnFNFkmBlVsrBpA)
    - å¾®ä¿¡å…¬ä¼—å·ã€ŒGitHubFunç½‘ç«™ã€
        - [2024-04-20ï¼Œè‹±ä¼Ÿè¾¾å¼€æºäººå·¥æ™ºèƒ½ä»£æ•°åº“ï¼šçº¿æ€§ä»£æ•°å­ä¾‹ç¨‹çš„ CUDA æ¨¡æ¿](https://mp.weixin.qq.com/s/CwTnG89-tc1HaapvbU0D6g)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œå¤§æ¨¡å‹ç”Ÿæ€åœˆã€
        - [2024-03-18ï¼ŒLLMç™¾å€æ¨ç†åŠ é€Ÿä¹‹é‡åŒ–ç¯‡](https://mp.weixin.qq.com/s/jbpVBZLZ0AkrP7bacY5mKw)
        - [2024-03-22ï¼ŒLLMæ¨ç†ï¼šGPUèµ„æºå’Œæ¨ç†æ¡†æ¶é€‰æ‹©](https://mp.weixin.qq.com/s/qUaLOXZmk1xyGHGKX4ZtpQ)
        - [2024-03-27ï¼ŒLLM æ¨ç†åŠ é€Ÿæ–¹å¼æ±‡æ€»](https://mp.weixin.qq.com/s/IlaQw6Ut25NNoTZkxs63Vg)
        - [2024-04-26ï¼ŒLLMæ¨ç†é‡åŒ–ï¼šFP8 VS INT8](https://mp.weixin.qq.com/s/e7QZC1qNkETXNXZpcD9cRg)
        - [2024-04-28ï¼ŒNvidia GPUæ± åŒ–-è¿œç¨‹GPU](https://mp.weixin.qq.com/s/tFdtYy5L_0V85OTvlPVK0A)
        - [2024-05-01ï¼ŒNvidia Tensor Core åˆæ¢](https://mp.weixin.qq.com/s/VAuk2WdFqiW4ujV0A3-8HA)
        - [2024-05-24ï¼ŒPytorch æ˜¾å­˜ç®¡ç†æœºåˆ¶ä¸æ˜¾å­˜å ç”¨åˆ†ææ–¹æ³•](https://mp.weixin.qq.com/s/QufR1esHGc3qkwgW6sAM-Q)
        - [2024-06-02ï¼Œ[LLMæ¨ç†ä¼˜åŒ–][ä¸‡å­—]TensorRT-LLMéƒ¨ç½²è°ƒä¼˜-æŒ‡åŒ—](https://mp.weixin.qq.com/s/PGOleShWEjHCPpw1wuV7SA)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œè‹å“²ç®¡ç†å’¨è¯¢ã€
        - [2024-02-25ï¼Œè‹±ä¼Ÿè¾¾ï¼ˆNVIDAï¼‰å´›èµ·ä¸å¹³å‡¡ä¹‹è·¯--è€é»„å…¨çƒAIèŠ¯ç‰‡æ–°å¸å›½ç®€å²](https://mp.weixin.qq.com/s/4c8FtVeJmNlXL6akj5lj8A)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œåæ¥é‡è§AIã€
        - [2022-08-08ï¼Œã€æœºå™¨å­¦ä¹ ã€‘Kå‡å€¼èšç±»ç®—æ³•åŸç†](https://mp.weixin.qq.com/s/o9bl1M9G1cOSYzzTZ3eYxw)
        - [2022-08-11ï¼Œã€CUDAç¼–ç¨‹ã€‘åŸºäºCUDAçš„Kmeansç®—æ³•çš„ç®€å•å®ç°](https://mp.weixin.qq.com/s/2PfocGm9l84l5Jj1vYF5bg)
        - [2024-01-23ï¼Œã€CUDAç¼–ç¨‹ã€‘åŸºäº CUDA çš„ Kmeans ç®—æ³•çš„è¿›é˜¶å®ç°ï¼ˆä¸€ï¼‰](hhttps://mp.weixin.qq.com/s/5Kr8ltlzy1nL7aeGrETYvA)
        - [2024-01-24ï¼Œã€CUDAç¼–ç¨‹ã€‘åŸºäº CUDA çš„ Kmeans ç®—æ³•çš„è¿›é˜¶å®ç°ï¼ˆäºŒï¼‰](https://mp.weixin.qq.com/s/xPN5cupqt4B-JrX6KUNJrw)
        - [2024-04-08ï¼Œã€CUDAç¼–ç¨‹ã€‘CUDA ç»Ÿä¸€å†…å­˜](https://mp.weixin.qq.com/s/DynVo_Mu7pUQxRLHH3ii9Q)
        - [2024-08-06ï¼Œã€CUDAç¼–ç¨‹ã€‘cuBLAS åº“ä¸­çŸ©é˜µä¹˜æ³•å‚æ•°è®¾ç½®é—®é¢˜](https://mp.weixin.qq.com/s/MvTaIBfVW3gcwQtV2VjMTw)
        - [2025-04-08ï¼Œã€CUDAç¼–ç¨‹ã€‘Flash Attention CUDA ç®—å­çš„å®ç°æ€è·¯](https://mp.weixin.qq.com/s/ReVeJ9fNlWeyfP8CMLolkg)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œæ±Ÿå¤§ç™½ã€
        - [2023-09-06ï¼ŒGPUåº•å±‚ä¼˜åŒ–ï¼Œå¦‚ä½•è®©Transformeråœ¨GPUä¸Šè·‘å¾—æ›´å¿«ï¼Ÿ](https://mp.weixin.qq.com/s/Xdbkld6ZrJ7Q93PEOedBMA)
        - [2024-04-12ï¼Œæ·±å…¥æµ…å‡ºï¼ŒPyTorchæ¨¡å‹int8é‡åŒ–åŸç†æ‹†è§£](https://mp.weixin.qq.com/s/j2QS3LdudrrlyZYQkVrl5Q)
        - [2024-04-13ï¼ŒCUDAæ¨¡å‹éƒ¨ç½²å®æˆ˜ï¼Œè‡ªå·±å†™çš„CUDAçŸ©é˜µä¹˜æ³•èƒ½ä¼˜åŒ–åˆ°å¤šå¿«ï¼Ÿ](https://mp.weixin.qq.com/s/ySfGSHyLrW5cRG17-B14rQ)
        - [2024-04-22ï¼ŒCUDAç¼–ç¨‹ä¸­ï¼ŒTensor Coresçš„è¯¦ç»†æ‹†è§£](https://mp.weixin.qq.com/s/uDWOg9-pRudcvroZADsIbg)
        - [2024-06-22ï¼ŒFP8é‡åŒ–è§£è¯»ï¼Œ8bitä¸‹éƒ¨ç½²æœ€ä¼˜æ–¹æ¡ˆï¼Ÿ](https://mp.weixin.qq.com/s/5DdMXCRq7X6QkS2yXJqF7g)
        - [2024-06-26ï¼ŒCudaç¼–ç¨‹å®è·µï¼Œæˆ‘çš„ç¬¬ä¸€ä»½Cudaä»£ç ](https://mp.weixin.qq.com/s/JxpNDmDTiS-ctCCG-RY1nw)
        - [2025-01-03ï¼ŒAIé¡¹ç›®å·¥ç¨‹åŒ–ï¼ŒCUDAå¼€å‘å¿ƒå¾—æ±‡æ€»ï¼](https://mp.weixin.qq.com/s/9mSV2ZY6EdBAAbtgq4gkZg)
    - å¾®ä¿¡å…¬ä¼—å·ã€ŒTimåœ¨è·¯ä¸Šã€
        - [2024-03-25ï¼Œç†è§£NVIDIA GPU æ€§èƒ½ï¼šåˆ©ç”¨ç‡ä¸é¥±å’Œåº¦](https://mp.weixin.qq.com/s/4_An51JuRGWTU0dLgZYHpQ)
        - [2024-04-30ï¼ŒåŠ é€ŸçŸ©é˜µè®¡ç®—ï¼šè‹±ä¼Ÿè¾¾TensorCoreæ¶æ„æ¼”è¿›ä¸åŸç†æœ€å…¨è§£æ](https://mp.weixin.qq.com/s/dwT1Fl6F4V1MvWGgt1ac0Q)
        - [2024-05-15ï¼Œæ­ç§˜ Tensor Core åº•å±‚ï¼šå¦‚ä½•è®©AIè®¡ç®—é€Ÿåº¦é£è·ƒ](https://mp.weixin.qq.com/s/UL7CLWp3cmdUgGILr4iVzA)
        - [2024-05-27ï¼Œæµ…æGPUåˆ†å¸ƒå¼é€šä¿¡æŠ€æœ¯-PCleã€NVLinkã€NVSwitch](https://mp.weixin.qq.com/s/ZllBWNqBwiY-Cb0UFIkwVg)
        - [2024-09-09ï¼Œä½¿ç”¨Nsight Profilingå·¥å…·å¯¹å¤§æ¨¡å‹è¿›è¡Œæ€§èƒ½è°ƒä¼˜](https://mp.weixin.qq.com/s/1t22WJCtqaWTP6UvFRnFeg)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œæ½®è§‚ä¸–ç•Œã€
        - [2024-04-19ï¼ŒAI æ¨ç†ï¼šCPU çš„å´›èµ·](https://mp.weixin.qq.com/s/rpdCT1fj2E3GKknfygAWRw)
    - å¾®ä¿¡å…¬ä¼—å·ã€ŒDeepDrivingã€
        - [2023-07-21ï¼ŒAIæ¨¡å‹éƒ¨ç½² | TensorRTæ¨¡å‹INT8é‡åŒ–çš„Pythonå®ç°](https://mp.weixin.qq.com/s/IQTCUs8CcfgHxJCyV6cm3w)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œäººå·¥æ™ºèƒ½å¤§è®²å ‚ã€
        - [2024-05-11ï¼Œæˆ‘æ‰¾åˆ°äº†AlexNetå½“å¹´çš„æºä»£ç ï¼Œæ²¡ç”¨æ¡†æ¶ï¼Œä»é›¶æ‰‹æ’¸CUDA/C++](https://mp.weixin.qq.com/s/plxXG8y5QlxSionyjyPXqw)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œæœªæ¥ç§‘æŠ€æ½®ã€
        - [2024-04-10ï¼Œé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆCUDAä¼˜åŒ–å¯å®ç°æ€§èƒ½ç¿»å€æå‡](https://mp.weixin.qq.com/s/FtJpRrfnFACM37p9fQv3cw)
        - [2024-06-21ï¼Œè§£å¯†é«˜æ€§èƒ½è®¡ç®—ï¼šå¦‚ä½•ç”¨æµå’ŒKernelè§¦å‘æå‡GPUé€šä¿¡æ•ˆç‡](https://mp.weixin.qq.com/s/X3A8Dc_48oHMo2arPFqHoQ)
    - å¾®ä¿¡å…¬ä¼—å·ã€ŒAIé“ä¸Šã€
        - [2024-04-19ï¼Œè‹±ä¼Ÿè¾¾åšæŒäº†16å¹´çš„CUDAï¼Œåˆ°åº•æ˜¯ä»€ä¹ˆ](https://mp.weixin.qq.com/s/nsBxZe_UXdvwfQ7DmCFuQg)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œç§‘æŠ€è¯‘è§ˆã€
        - [2024-04-09ï¼Œ100è¡ŒCä»£ç é‡å¡‘æ·±åº¦å­¦ä¹ ï¼šç”¨çº¯C/CUDAæ‰“é€ çš„æç®€LLMè®­ç»ƒ](https://mp.weixin.qq.com/s/Th3RX3_FS5git0qJEcu4ZA)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œå°ç™½å­¦è§†è§‰ã€
        - [2024-03-29ï¼Œå›¾åƒé¢„å¤„ç†åº“CV-CUDAå¼€æºäº†ï¼Œæ‰“ç ´é¢„å¤„ç†ç“¶é¢ˆï¼Œæå‡æ¨ç†ååé‡20å¤šå€](https://mp.weixin.qq.com/s/Zn4yI1xu2TuXZkJCzQt_yA)
        - [2025-01-24ï¼Œç®—æ³•å²—å¹³æ—¶éœ€è¦è‡ªå·±å†™cudaå—ï¼Ÿ](https://mp.weixin.qq.com/s/L_SoagU1V0lZA75-7qef3A)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œå¡å·´æ–¯ã€
        - [2024-02-26ï¼ŒGPUï¼ˆä¸€ï¼‰GPUç®€ä»‹](https://mp.weixin.qq.com/s/V4mMjzQ261kk6qmyH-STUQ)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œç ç –æ‚å½¹ã€
        - [2024-04-03ï¼Œã€CUDAã€‘ä¸€æ–‡è®²æ¸…æµä¸å¹¶å‘ï¼Œè®²ä¸æ¸…æˆ‘é‡è®²](https://mp.weixin.qq.com/s/-eJOdG7A-bvum9GFkiNIoQ)
        - [2024-05-02ï¼Œã€ã€CUDAã€‘ä¸€æ–‡è®²æ¸…å…±äº«å†…å­˜å’Œå¸¸é‡å†…å­˜](https://mp.weixin.qq.com/s/qcynKSz2zQQQ2Ylk_sSorw)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œæ˜Ÿæƒ³æ³•ã€
        - [2022-09-19ï¼Œé›¶çŸ¥è¯†è¯æ˜ - FPGA vs. GPU](https://mp.weixin.qq.com/s/SjoeQHboe2RI4EJKfpMjKw)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œå¤ªæå›¾å½¢ã€
        - [2022-06-16ï¼Œå‡å°‘é‡å¤é€ è½®å­ï¼Œå¸®ä½ è§£æ”¾ç”Ÿäº§åŠ›çš„ã€Œå°çŸ©é˜µåŠŸèƒ½ã€æ¥å•¦ï¼](https://mp.weixin.qq.com/s/5PGXUxcUMSfsbVbrennUFA)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œç¡…æ˜ŸäººProã€
        - [2024-06-03ï¼Œé»„ä»å‹‹ï¼šè‹±ä¼Ÿè¾¾å°†ä¸€å¹´æ¨ä¸€æ¬¾å…¨æ–°èŠ¯ç‰‡ï¼Œæ²¡æœ‰è‹±ä¼Ÿè¾¾å°±æ²¡æœ‰ä»Šå¤©AIçš„ä¸€åˆ‡ï¼ˆé™„æœ€æ–°æ¼”è®²å…¨æ–‡ï¼‰](https://mp.weixin.qq.com/s/Uc6heL537JNn63JXsDSVOg)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œ3Dè§†è§‰ä¹‹å¿ƒã€
        - [2024-06-01ï¼Œä¼ ç»ŸSLAMä½¿ç”¨CUDAåŠ é€Ÿæ˜¯å¦æœ‰æ¯”è¾ƒå¤§çš„ä¼˜åŠ¿å‘¢ï¼Ÿ](https://mp.weixin.qq.com/s/5SlVcsDJd8VvABo6wCe4AQ)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œä¸­å›½ä¼ä¸šå®¶æ‚å¿—ã€
        - [2024-06-01ï¼Œé»„ä»å‹‹ï¼šä¸å–œæ¬¢è£å‘˜ï¼Œæˆ‘å®æ„¿â€œæŠ˜ç£¨â€ä»–ä»¬ï½œä¸­ä¼èè¯»](https://mp.weixin.qq.com/s/8jIgJPsWuCnj92wa61llSw)
    - å¾®ä¿¡å…¬ä¼—å·ã€ŒCSharpä¸è¾¹ç¼˜æ¨¡å‹éƒ¨ç½²ã€
        - [2024-06-04ï¼Œä½¿ç”¨ TensorRT C++ API è°ƒç”¨GPUåŠ é€Ÿéƒ¨ç½² YOLOv10 å®ç° 500FPS æ¨ç†é€Ÿåº¦â€”â€”å¿«åˆ°é£èµ·ï¼ï¼](https://mp.weixin.qq.com/s/yijeZtkRhbQxuSE1AsyUhA)
    - å¾®ä¿¡å…¬ä¼—å·ã€ŒNeuralTalkã€
        - [2023-06-16ï¼ŒSIMD æŒ‡ä»¤é›†ä¸æ•°æ®å¹¶è¡Œç¨‹åº](https://mp.weixin.qq.com/s/dgTtEY5NZh-npQ6KN2WoaA)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œå°å´æŒç»­å­¦ä¹ AIã€
        - [2023-06-12ï¼Œä¸ºCUDA Kernelé€‰æ‹©åˆé€‚çš„grid_sizeå’Œblock_size](https://mp.weixin.qq.com/s/Je0ZCPv6RKacX__TFL1y4A)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œå¤§æ¨¡å‹æ–°è§†ç•Œã€
        - [2024-06-20ï¼Œå¤§æ¨¡å‹é‡åŒ–æ€§èƒ½è¯„ä»·æŒ‡æ ‡](https://mp.weixin.qq.com/s/S76alcWhBdM5gWJvT0udAQ)
        - [2024-06-24ï¼ŒFP8 é‡åŒ–åŸºç¡€ - è‹±ä¼Ÿè¾¾](https://mp.weixin.qq.com/s/MnOze4BGP-a7Un4K0sakbg)
        - [2024-07-05ï¼ŒèŠèŠå¤§æ¨¡å‹æ¨ç†ä¸­çš„åˆ†ç¦»å¼æ¨ç†](https://mp.weixin.qq.com/s/4vO3j4LXcmsZ97WfabZzfA)
        - [2024-07-11ï¼ŒFP8 ä½ç²¾åº¦è®­ç»ƒï¼šTransformer Engine ç®€æ](https://mp.weixin.qq.com/s/r836OOVNo9z_HHTX-MtO-A)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œé‡å­ä½ã€
        - [2024-06-17ï¼Œé»„ä»å‹‹è‡´æ¯•ä¸šç”Ÿï¼šå‹‡äºè¿›å…¥0äº¿ç¾å…ƒå¸‚åœºï¼Œå¸Œæœ›ä½ èƒ½æ‰¾åˆ°è‡ªå·±çš„GPU](https://mp.weixin.qq.com/s/m7ySazb1DrsLUQHqSW37mg)
    - å¾®ä¿¡å…¬ä¼—å·ã€ŒHPCæ™ºèƒ½æµä½“å¤§æœ¬è¥ã€
        - [2024-03-26ï¼ŒGPU ä¸Š GEMM çš„æ€§èƒ½ä¼˜åŒ–æŒ‡æ ‡](https://mp.weixin.qq.com/s/0sNkjkE9LJ3o6_w5uR_XgA)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œäººå·¥æ™ºèƒ½å‰æ²¿è®²ä¹ ã€
        - [2023-07-06ï¼Œã€ä»–å±±ä¹‹çŸ³ã€‘CUDA SGEMMçŸ©é˜µä¹˜æ³•ä¼˜åŒ–ç¬”è®°â€”â€”ä»å…¥é—¨åˆ°cublas](https://mp.weixin.qq.com/s/0iAbHZ-hN6Mj2c2j2Uw03w)
    - å¾®ä¿¡å…¬ä¼—å·ã€ŒAIè®©ç”Ÿæ´»æ›´ç¾å¥½ã€
        - [2024-07-06ï¼ŒThrust åº“ï¼šè®© C++ å¹¶è¡Œè®¡ç®—é£è·ƒ](https://mp.weixin.qq.com/s/GtMolxSU-VKjs0cheMOykg)
    - å¾®ä¿¡å…¬ä¼—å·ã€ŒNEæ—¶ä»£æ™ºèƒ½è½¦ã€
        - [2024-07-09ï¼Œç†æƒ³æ˜¯å¦‚ä½•å°†è§†è§‰è¯­è¨€å¤§æ¨¡å‹éƒ¨ç½²åˆ°Orin-Xä¸Šçš„ï¼Ÿ](https://mp.weixin.qq.com/s/EBnfgXY_fxlQI-7eykwqZA)
    - å¾®ä¿¡å…¬ä¼—å·ã€ŒOpenCVä¸AIæ·±åº¦å­¦ä¹ ã€
        - [2024-07-08ï¼Œå®æˆ˜ | YOLOv8ä½¿ç”¨TensorRTåŠ é€Ÿæ¨ç†æ•™ç¨‹ï¼ˆæ­¥éª¤ + ä»£ç ï¼‰](https://mp.weixin.qq.com/s/VcUifHycY9aw99d3WD1h1w)
        - [2024-07-10ï¼ŒOpenCVä½¿ç”¨CUDAåŠ é€Ÿèµ„æ–™æ±‡æ€»(pdf+è§†é¢‘+æºç )](https://mp.weixin.qq.com/s/o-AECBLDucxVLr1Q0yxZ_g)
    - å¾®ä¿¡å…¬ä¼—å·ã€ŒInfiniTensorã€
        - [2024-07-19ï¼Œsoftmaxç®—å­å¼€å‘ä»‹ç»](https://mp.weixin.qq.com/s/KT5rzTj3jNj_aw5hXm-kTw)
        - [2024-07-24ï¼ŒCUDAå®ç°matmulçš„å¹¶è¡Œç­–ç•¥](https://mp.weixin.qq.com/s/U_-NnW2yx3jnc1vCfEi1Cg)
        - [2024-07-27ï¼Œflash attentionçš„CUDAç¼–ç¨‹](https://mp.weixin.qq.com/s/RRP45uuC-KgKZ88bzTLgUQ)
        - [2024-07-30ï¼ŒCUDAå®ç°è§„çº¦çš„å¹¶è¡Œç­–ç•¥](https://mp.weixin.qq.com/s/OslgzL-qXV9KNrzgb3Fsvg)
    - å¾®ä¿¡å…¬ä¼—å·ã€ŒGeekSavvyã€
        - [2024-03-19ï¼Œå²ä¸Šæœ€å¼ºèŠ¯ç‰‡æ¨å‡ºï¼è‹±ä¼Ÿè¾¾å‘å¸ƒæ–°ä¸€ä»£BlackWell GPU](https://mp.weixin.qq.com/s/Ld2P68IBzFTDYrx3rY8rNw)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œé˜¿æœ¨å®éªŒå®¤ã€
        - [2024-07-31ï¼Œæ­è½½è‹±ä¼Ÿè¾¾Jetson Orinçš„Allspark 2å…¨æ–°äº®ç›¸ï¼Œç®—åŠ›é«˜è¾¾100TOPSï¼](https://mp.weixin.qq.com/s/hjU3eH1LAvxVsOHWZXFAyQ)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œåƒæœå†»ä¸åæœå†»çš®ã€
        - [2024-08-09ï¼Œå¦‚ä½•æŠŠ PyTorch çš„ GPU åˆ©ç”¨ç‡æå‡åˆ° 100% ?](https://mp.weixin.qq.com/s/9HSZppiFjypwu-TttbxqMQ)
        - [2024-08-26ï¼ŒåŸºäº NVIDIA TensorRT-LLM çš„å¤§è¯­è¨€æ¨¡å‹è°ƒåº¦æ–¹æ³•](https://mp.weixin.qq.com/s/nDSYDsmAtozmHNzLsx4KzQ)
    - å¾®ä¿¡å…¬ä¼—å·ã€ŒAIå¤§æ¨¡å‹å®éªŒå®¤ã€
        - [2024-03-19ï¼ŒNvidiaæ¨å‡ºBlackwell B200 GPUï¼Œæ˜¯ç›®å‰æœ€å¼ºçš„äººå·¥æ™ºèƒ½èŠ¯ç‰‡](https://mp.weixin.qq.com/s/SPgz7q_Ycr2lNxeH0WoNEQ)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œç§‘æŠ€æœ€å‰çº¿ã€
        - [2024-03-19ï¼Œèƒ½è¶…è¶Šè‹±ä¼Ÿè¾¾çš„åªæœ‰è‹±ä¼Ÿè¾¾](https://mp.weixin.qq.com/s/ORMH0JKdeS-i_DAjdcNvXg)
    - å¾®ä¿¡å…¬ä¼—å·ã€ŒAIèŒƒå„¿ã€
        - [2024-03-20ï¼ŒNVIDIA ä¸ Blackwell ä¸€èµ·æ”¹å†™æ‘©å°”å®šå¾‹](https://mp.weixin.qq.com/s/3ffnbFdnXV7lQylzo_3o9w)
    - å¾®ä¿¡å…¬ä¼—å·ã€ŒDataFunTalkã€
        - [2024-03-15ï¼ŒNVIDIAå¤§è¯­è¨€æ¨¡å‹è½åœ°çš„å…¨æµç¨‹è§£æ](https://mp.weixin.qq.com/s/mhGcW8FqLigBeePlRZGBDg)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œå…ˆè¿›ç¼–è¯‘å®éªŒå®¤ã€
        - [2025-01-20ï¼Œç©è½¬GPUåŠ é€Ÿï¼šCUDAç¼–ç¨‹ä¼˜åŒ–ä¹‹æ—…å¼€å¯](https://mp.weixin.qq.com/s/tke62pNpdM1AW4VPt8MHlg)
        - [2025-02-04ï¼ŒCUDAä¼˜åŒ–ç§˜ç±ï¼šè§£é”å†…å­˜æ€§èƒ½çš„å››å¤§å…³é”®](https://mp.weixin.qq.com/s/fI7mBYM6SrexCgZRjoccCw)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œè®³ç–¾å¿ŒåŒ»-noteã€
        - [2024-12-21ï¼Œå›½å¤–æ·±åº¦å­¦ä¹ å·¥ç¨‹å¸ˆåˆ†äº«ï¼šä»é›¶å¼€å§‹é‡ç° PyTorchï¼ˆæ”¯æŒ GPUï¼‰è¶…è¯¦ç»†](https://mp.weixin.qq.com/s/FAfADW1jPt40RFxwyfCGJQ)
    - å¾®ä¿¡å…¬ä¼—å·ã€ŒDeepHub IMBAã€
        - [2023-10-22ï¼Œä½¿ç”¨TensorRT-LLMè¿›è¡Œé«˜æ€§èƒ½æ¨ç†](https://mp.weixin.qq.com/s/pIZ9ceJzTG8kMZMn1m5oQw)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œzarbotã€
        - [2024-07-25ï¼ŒTensor-001 çŸ©é˜µä¹˜æ³•åˆ†å—ä¹˜æ³•æ¦‚è¿°](https://mp.weixin.qq.com/s/61CzHIZcX6DSUBcjbgr2eQ)
        - [2024-07-25ï¼ŒTensor-002 çŸ©é˜µä¹˜æ³•ä¼˜åŒ–](https://mp.weixin.qq.com/s/tpzBq_rZdZt3whqw0txCDQ)
        - [2024-08-03ï¼ŒTensor-003 TensorCoreæ¶æ„](https://mp.weixin.qq.com/s/xdC8SlMQb7O0S3E5VXXpRQ)
        - [2024-08-10ï¼ŒTensor-004 TensorCoreç¼–ç¨‹åŠä¼˜åŒ–](https://mp.weixin.qq.com/s/eB6nbPpVwPqLyo9YMiw93w)
        - [2024-08-20ï¼ŒTensor-005 CUTLASSç®€ä»‹](https://mp.weixin.qq.com/s/LaK8jucDgrRxPlDMq-aTvw)
        - [2024-08-22ï¼ŒTensor-006 AIè½¯ç¡¬ä»¶äº¤äº’ç•Œé¢: å¯ç»„åˆçš„Kernel](https://mp.weixin.qq.com/s/JA8wCZFAlkTVwfW8tmke9w)
        - [2024-08-24ï¼ŒTensor-007 Cute Layoutç®€ä»‹](https://mp.weixin.qq.com/s/3uJtWOyhB9PX72ynocjVZg)
        - [2024-08-29ï¼ŒTensor-008 CuTe Layoutä»£æ•°](https://mp.weixin.qq.com/s/CWBxDkd1mj0WPmSAUHSHhg)
        - [2024-09-09ï¼ŒTensor-009 Cute Tensor](https://mp.weixin.qq.com/s/sd7HY68Z-UZiqlv_PfoayA)
        - [2024-09-14ï¼ŒTensor-010 Tensor Copy](https://mp.weixin.qq.com/s/90D7Ro_kkbM1rj4szyeDRw)
    - å¾®ä¿¡å…¬ä¼—å·ã€ŒInfraç ”ä¹ ç¤¾ã€
        - [2025-03-24ï¼Œcuda-python ä½¿ç”¨ä¸cuTicleä»‹ç»](https://mp.weixin.qq.com/s/Q_fSDMy6FNPqgmAW_kaPUw)



    - [çŸ¥ä¹ã€Œç´«æ°”ä¸œæ¥ã€](https://www.zhihu.com/people/zi-qi-dong-lai-1)
        - [2023-09-02ï¼ŒCUDAï¼ˆä¸€ï¼‰ï¼šCUDA ç¼–ç¨‹åŸºç¡€](https://zhuanlan.zhihu.com/p/645330027)
        - [2023-09-09ï¼ŒCUDAï¼ˆäºŒï¼‰ï¼šGPUçš„å†…å­˜ä½“ç³»åŠå…¶ä¼˜åŒ–æŒ‡å—](https://zhuanlan.zhihu.com/p/654027980)
        - [2023-09-29ï¼ŒCUDAï¼ˆä¸‰ï¼‰ï¼šé€šç”¨çŸ©é˜µä¹˜æ³•ï¼šä»å…¥é—¨åˆ°ç†Ÿç»ƒ](https://zhuanlan.zhihu.com/p/657632577)
        - [2024-04-29ï¼Œops(1)ï¼šLayerNorm ç®—å­çš„ CUDA å®ç°ä¸ä¼˜åŒ–](https://zhuanlan.zhihu.com/p/694974164)
        - [2024-04-30ï¼Œops(2)ï¼šSoftMaxç®—å­çš„ CUDA å®ç°](https://zhuanlan.zhihu.com/p/695307283)
        - [2024-05-01ï¼Œops(3)ï¼šCross Entropy çš„ CUDA å®ç°](https://zhuanlan.zhihu.com/p/695594396)
        - [2024-05-01ï¼Œops(4)ï¼šAdamW ä¼˜åŒ–å™¨çš„ CUDA å®ç°](https://zhuanlan.zhihu.com/p/695611950)
        - [2024-05-02ï¼Œops(5)ï¼šæ¿€æ´»å‡½æ•°ä¸æ®‹å·®è¿æ¥çš„ CUDA å®ç°](https://zhuanlan.zhihu.com/p/695703671)
        - [2024-05-03ï¼Œops(6)ï¼šembedding å±‚ä¸ LM head å±‚çš„ CUDA å®ç°](https://zhuanlan.zhihu.com/p/695785781)
        - [2024-05-06ï¼Œops(7)ï¼šself-attention çš„ CUDA å®ç°åŠä¼˜åŒ– (ä¸Š)](https://zhuanlan.zhihu.com/p/695898274)
        - [2024-05-08ï¼Œops(8)ï¼šself-attention çš„ CUDA å®ç°åŠä¼˜åŒ– (ä¸‹)](https://zhuanlan.zhihu.com/p/696197013)
        - [2024-05-14ï¼ŒCUDAï¼ˆå››ï¼‰ï¼šä½¿ç”¨ CUDA å®ç° Transformer ç»“æ„](https://zhuanlan.zhihu.com/p/694416583)
    - [çŸ¥ä¹ã€Œæ˜¯èªæ˜è²‚å–ã€](https://www.zhihu.com/people/cmd23333)
        - [2024-02-18ï¼Œã€Šé«˜æ€§èƒ½å¹¶è¡Œç¼–ç¨‹ä¸ä¼˜åŒ–ã€‹è¯¾ç¨‹ç¬”è®°ç›®å½•](https://zhuanlan.zhihu.com/p/671684145)


    - [Medium Blog](https://medium.com/)
        - [2024-05-15ï¼ŒRecreating PyTorch from Scratch (with GPU Support and Automatic Differentiation)](https://medium.com/towards-data-science/recreating-pytorch-from-scratch-with-gpu-support-and-automatic-differentiation-8f565122a3cc)



  - ### Triton Blogs

    - å¾®ä¿¡å…¬ä¼—å·ã€Œæ™ºæºç ”ç©¶é™¢ã€
        - [2024-07-09ï¼Œæ™ºæºæ‰“é€ åŸºäºTritonçš„å¤§æ¨¡å‹ç®—å­åº“ï¼ŒåŠ©åŠ›AIèŠ¯ç‰‡è½¯ç¡¬ä»¶ç”Ÿæ€å»ºè®¾](https://mp.weixin.qq.com/s/03EYxAyu4uWI4tjMkFZjSQ)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œæ™ºæºFlagOpenã€
        - [2024-09-06ï¼Œæ™ºæºæ‰“é€ åŸºäºTritonçš„å¤§æ¨¡å‹ç®—å­åº“ï¼ŒåŠ©åŠ›AIèŠ¯ç‰‡è½¯ç¡¬ä»¶ç”Ÿæ€å»ºè®¾](https://mp.weixin.qq.com/s/QMzgj-h6q2mZHokS5QpXmg)
        - [2024-09-18ï¼ŒTritonå¤§ä¼š@ç¡…è°·ï¼šèŠ¯ç‰‡ã€AIå¤§å‚é½ç«™å°](https://mp.weixin.qq.com/s/euX2nxQ4lhG6yaLYMugyrw)
        - [2024-11-20ï¼ŒTritonæ´»åŠ¨ï½œTritonä¸­å›½ç¤¾åŒºè´¡çŒ®è€…èŒ¶è¯ä¼š](https://mp.weixin.qq.com/s/ASU1Z9B21kD8WJMhZ5E_ZA)
        - [2024-12-04ï¼ŒTritonä¸­å›½ç¤¾åŒºè´¡çŒ®è€…èŒ¶è¯ä¼šåœ†æ»¡è½åœ°](https://mp.weixin.qq.com/s/vtSMaGgp_Uxo_QvmmV7sjg)
        - [2024-12-10ï¼ŒTritonå…¥é—¨å®è·µ | ç®—å­æ€§èƒ½ä¼˜åŒ–ï¼šè‡ªåŠ¨è°ƒä¼˜çš„è‰ºæœ¯](https://mp.weixin.qq.com/s/txtyci4MUCVNpTZwF6fS4Q)
        - [2024-12-19ï¼Œæ™ºæºå¤§æ¨¡å‹é€šç”¨ç®—å­åº“FlagGemså››å¤§èƒ½åŠ›å‡çº§ï¼Œä¸ºAIç³»ç»Ÿå¼€æºç”Ÿæ€æ³¨å…¥æ–°æ´»åŠ›](https://mp.weixin.qq.com/s/2X5DyY1MbYhz6zVusPpKww)
        - [2025-01-13ï¼ŒTritonç”Ÿæ€ | çªç ´CUDAæŸç¼šï¼Œæ‹¥æŠ±å¼€æ”¾ä¸å¤šå…ƒçš„æœªæ¥](https://mp.weixin.qq.com/s/IX3kalsNUcXJ_JgDWKJ1iQ)
    - å¾®ä¿¡å…¬ä¼—å·ã€ŒPyTorchã€
        - [2025-01-24ï¼ŒTritonåŠ é€Ÿ2DåŠ¨æ€å—é‡åŒ–Float8 GEMM](https://mp.weixin.qq.com/s/jixXXEBId9PHhK40L77bEQ)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œæ‘©å°”çº¿ç¨‹ã€
        - [2024-07-18ï¼Œæ‘©å°”çº¿ç¨‹ Ã— æ™ºæºç ”ç©¶é™¢ï½œå®ŒæˆåŸºäºTritonçš„å¤§æ¨¡å‹ç®—å­åº“é€‚é…](https://mp.weixin.qq.com/s/84LKQ4Xo1RSdNoFJG0tUmg)
        - [2024-11-05ï¼Œå¼€æºvLLM-MUSAï½œæ‘©å°”çº¿ç¨‹æŒç»­åŠ é€ŸåŸºäºå›½äº§GPUçš„AIå¤§æ¨¡å‹æ¨ç†å¼€å‘](https://mp.weixin.qq.com/s/5YLCXLlkbZ9WwoaiSIFJhA)
        - [2024-11-12ï¼Œå¼€æºMUTLASSï½œæ‘©å°”çº¿ç¨‹åŠ é€ŸåŸºäºå›½äº§GPUçš„ç®—å­å¼€å‘ä»¥åŠç®—æ³•åˆ›æ–°](https://mp.weixin.qq.com/s/Nm8BKAJD_ibht8pG9CfEFQ)
    - å¾®ä¿¡å…¬ä¼—å·ã€ŒHyperAIè¶…ç¥ç»ã€
        - [2024-10-14ï¼Œé¦–ä¸ªå®Œæ•´ Triton ä¸­æ–‡æ–‡æ¡£ä¸Šçº¿ï¼å¼€å¯ GPU æ¨ç†åŠ é€Ÿæ–°æ—¶ä»£](https://mp.weixin.qq.com/s/ytuxAMlpss5Il_cWf8Y1YQ)
    - å¾®ä¿¡å…¬ä¼—å·ã€ŒInfiniTensorã€
        - [2024-08-22ï¼ŒOpenAI Triton ç®€ä»‹ï¼ˆä¸€ï¼‰](https://mp.weixin.qq.com/s/WWPoIcqUDSYQsUI9HbN5-g)
        - [2024-10-24ï¼ŒOpenAI Triton ç®€ä»‹ï¼ˆäºŒï¼‰](https://mp.weixin.qq.com/s/XVzfsVFWDsteOIXAkqTYIw)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œåƒæœå†»ä¸åæœå†»çš®ã€
        - [2023-05-22ï¼Œæ¨¡å‹æ¨ç†æœåŠ¡åŒ–æ¡†æ¶Tritonä¿å§†å¼æ•™ç¨‹ï¼ˆä¸€ï¼‰ï¼šå¿«é€Ÿå…¥é—¨ ](https://mp.weixin.qq.com/s/YES9OO9NX6-HnzR-pvfFyQ)
        - [2023-06-02ï¼Œæ¨¡å‹æ¨ç†æœåŠ¡åŒ–æ¡†æ¶Tritonä¿å§†å¼æ•™ç¨‹ï¼ˆäºŒï¼‰ï¼šæ¶æ„è§£æ](https://mp.weixin.qq.com/s/BVKLsQ9GBN_VqStdjielrA)
        - [2023-06-03ï¼Œæ¨¡å‹æ¨ç†æœåŠ¡åŒ–æ¡†æ¶Tritonä¿å§†å¼æ•™ç¨‹ï¼ˆä¸‰ï¼‰ï¼šå¼€å‘å®è·µ](https://mp.weixin.qq.com/s/2jAw7tf4Pvd1o1AmukTiDw)
    - å¾®ä¿¡å…¬ä¼—å·ã€ŒGiantPandaCVã€
        - [2024-01-22ï¼Œã€BBufçš„CUDAç¬”è®°ã€‘åä¸‰ï¼ŒOpenAI Triton å…¥é—¨ç¬”è®°ä¸€](https://mp.weixin.qq.com/s/RMR_n1n6nBqpdMl6tdd7pQ)
        - [2024-10-08ï¼Œã€ç¿»è¯‘ã€‘ã€PyTorch å¥‡æŠ€æ·«å·§ã€‘FlexAttetion åŸºäºTritonæ‰“é€ çµæ´»åº¦æ‹‰æ»¡çš„Attention](https://mp.weixin.qq.com/s/KJUk-jmwGPrJvVuLQ44DyQ)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œæ–°æ™ºå…ƒã€
        - [2024-09-06ï¼ŒPyTorchå®˜å®£ï¼šå‘Šåˆ«CUDAï¼ŒGPUæ¨ç†è¿æ¥TritonåŠ é€Ÿæ–°æ—¶ä»£](https://mp.weixin.qq.com/s/AiViRkOOpmIUm8DfRUu4JA)
    - å¾®ä¿¡å…¬ä¼—å·ã€ŒCVæŠ€æœ¯æŒ‡å—ã€
        - [2024-09-08ï¼ŒPyTorchå®˜å®£ï¼šå‘Šåˆ«CUDAï¼ŒGPUæ¨ç†è¿æ¥TritonåŠ é€Ÿæ–°æ—¶ä»£](https://mp.weixin.qq.com/s/TABVE-pvXqNmsnkdkv_9MA)
    - å¾®ä¿¡å…¬ä¼—å·ã€ŒAIæ—¶ä»£çª—å£ã€
        - [2024-09-10ï¼Œä¸ä¾èµ–CUDAçš„å¤§æ¨¡å‹æ¨ç†å·²ç»å®ç°](https://mp.weixin.qq.com/s/TrzwSBQ301Grcpye9-pt1Q)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œå…ˆè¿›ç¼–è¯‘å®éªŒå®¤ã€
        - [2025-01-04ï¼Œæ¢ç´¢ Triton ç¼–ç¨‹å¯†ç ï¼šè¯­æ³•ä¸å®è·µæŒ‡å—å¤§æ­ç§˜](https://mp.weixin.qq.com/s/e2iZNBdiBPRmF_8GrLCKfw)
        - [2025-01-06ï¼Œæ·±å…¥Tritonæºç ï¼šæ­å¼€AIåŠ é€Ÿå¼•æ“çš„ç¥ç§˜é¢çº±ï¼](https://mp.weixin.qq.com/s/tSQecwSrpfJVVxD4_4blWQ)

    - [çŸ¥ä¹ã€ŒSoaringã€](https://www.zhihu.com/people/soaring-52-57)
        - [2024-05-14ï¼ŒOpenAI Triton å…¥é—¨](https://zhuanlan.zhihu.com/p/697626885)


  - ### TVM Blogs

    - å¾®ä¿¡å…¬ä¼—å·ã€Œå°å–µå­¦AIã€
        - [2023-05-04ï¼Œå®æˆ˜ | TVMä¼˜åŒ–Pytorchæ¨¡å‹](https://mp.weixin.qq.com/s/I-YXPJel_g0rGS9Vy8BE3Q)



  - ### MLIR Blogs

    - å¾®ä¿¡å…¬ä¼—å·ã€ŒGiantPandaCVã€
        - [2022-05-23ï¼ŒåŸºäº MLIR å®Œæˆå¯¹ GEMM çš„ç¼–è¯‘ä¼˜åŒ– ä¸­è‹±è§†é¢‘ä¸Šï¼Œä¸­éƒ¨åˆ†](https://mp.weixin.qq.com/s/9wyM3hKsJA0YxFsms1Rpuw)
        - [2023-06-25ï¼ŒMLIR_å¯¹è‡ªå®šä¹‰IR Dialectç¼–å†™bufferization pass](https://mp.weixin.qq.com/s/3aHwYDkI9K3u-10v6-9iVA)




  - ### HPC Blogs

    - å¾®ä¿¡å…¬ä¼—å·ã€ŒRVBoardsã€
        - [2021-03-23ï¼Œå¼ å…ˆè½¶åšå£«ï¼šOpenBLASé¡¹ç›®ä¸çŸ©é˜µä¹˜æ³•ä¼˜åŒ–](https://mp.weixin.qq.com/s/20SX_FL4cEDUx9pDJpOxnA)
    - å¾®ä¿¡å…¬ä¼—å·ã€ŒçŒ¿ç¦¹å®™ã€
        - [2023-11-11ï¼Œ æœ±æ‡¿ï¼šHPCä¹‹çŸ©é˜µä¹˜æ³•é«˜æ€§èƒ½å®éªŒæŠ¥å‘Š](https://mp.weixin.qq.com/s/WoDacoBqAJeV4PgNGtDq_A)
    - å¾®ä¿¡å…¬ä¼—å·ã€ŒNeuralTalkã€
        - [2023-06-16ï¼ŒSIMD æŒ‡ä»¤é›†ä¸æ•°æ®å¹¶è¡Œç¨‹åº](https://mp.weixin.qq.com/s/dgTtEY5NZh-npQ6KN2WoaA)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œæœ‰é™å…ƒè¯­è¨€ä¸ç¼–ç¨‹ã€
        - [2024-05-21ï¼Œå¹¶è¡Œè®¡ç®—ï¼šè¶…çº§å¤§è„‘èƒŒåçš„é­”æœ¯å¸ˆ](https://mp.weixin.qq.com/s/GnnJtXr6BZrnGsHJB-a-ag)
        - [2024-06-29ï¼ŒBLASç®€ä»‹ï¼šåŸºäºFortrançš„é«˜æ€§èƒ½çŸ©é˜µè®¡ç®—åŸºç¡€åº“](https://mp.weixin.qq.com/s/FXkxeezDVEY7asjl_PWX1g)
        - [2024-07-08ï¼ŒLAPACKç®€ä»‹ï¼šåŸºäºFortrançš„é«˜æ€§èƒ½çº¿æ€§ä»£æ•°å·¥å…·ç®±](https://mp.weixin.qq.com/s/iAxHrRFmVtcpX8otZytHvw)
    - å¾®ä¿¡å…¬ä¼—å·ã€Œé¸ŸçªèŠæŠ€æœ¯ã€
        - [2024-07-12ï¼Œä½¿ç”¨SIMDä¼˜åŒ–äºŒå‰æœç´¢æ ‘](https://mp.weixin.qq.com/s/u8BcfQKmtWIB86B4GetULQ)
    - å¾®ä¿¡å…¬ä¼—å·ã€ŒOpenCVä¸AIæ·±åº¦å­¦ä¹ ã€
        - [2024-06-21ï¼ŒYOLOv10åœ¨PyTorchå’ŒOpenVINOä¸­æ¨ç†å¯¹æ¯”](https://mp.weixin.qq.com/s/xZ4HlfBPXFbf8OPxmXwbrQ)

    - [çŸ¥ä¹ã€Œç™½ç‰›ã€](https://www.zhihu.com/people/huan-jun-81)
        - [2023-05-04ï¼ŒOpenBLAS gemmä»é›¶å…¥é—¨](https://zhuanlan.zhihu.com/p/65436463)
    - [çŸ¥ä¹ã€Œåº„ç¢§æ™¨ã€](https://www.zhihu.com/people/zhuang-chen-84-13)
        - [2021-01-22ï¼Œå¤šçº¿ç¨‹ GEMM è®ºæ–‡ ç¬”è®°](https://zhuanlan.zhihu.com/p/346254572)
    - [çŸ¥ä¹ã€ŒOeuFcoqueã€](https://www.zhihu.com/people/fsybdh)
        - [2020-04-12ï¼Œé«˜æ€§èƒ½è®¡ç®—ç®€ä»‹ï¼ˆä¸€ï¼‰ï¼šåˆæ­¥åˆ†æï¼ŒBLASï¼ŒBLISç®€ä»‹](https://zhuanlan.zhihu.com/p/129187064)
    - [çŸ¥ä¹ã€Œèµµå°æ˜12138ã€](https://www.zhihu.com/people/zhao-qi-ming-67)
        - [2022-10-26ï¼Œå¹¶è¡Œè®¡ç®—-canonç®—æ³•ï¼šçŸ©é˜µç›¸ä¹˜](https://zhuanlan.zhihu.com/p/577512867)
    - [çŸ¥ä¹ã€Œzeroã€](https://www.zhihu.com/people/zero-35-40)
        - [2021-12-18ï¼Œç¨ å¯†çŸ©é˜µä¹˜003(gemm)-OpenBLASå’ŒBLISåˆ†å—ç­–ç•¥](https://zhuanlan.zhihu.com/p/446908156)
    - [çŸ¥ä¹ã€Œä¸¥å¿»æºã€](https://www.zhihu.com/people/yan-xin-kai-38)
        - [2022-03-31ï¼Œæ–¯å¦ç¦CS217(ä¸‰)GEMMè®¡ç®—åŠ é€Ÿ](https://zhuanlan.zhihu.com/p/280771849)
    - [é»æ˜ç°çƒ¬ åšå®¢](https://zhenhuaw.me/)
        - [2019-06-12ï¼Œé€šç”¨çŸ©é˜µä¹˜ï¼ˆGEMMï¼‰ä¼˜åŒ–ç®—æ³•](http://zhenhuaw.me/blog/2019/gemm-optimization.html)
    - [Modular Blog](https://www.modular.com/blog)
        - [2023-03-23ï¼ŒAIâ€™s compute fragmentation: what matrix multiplication teaches us](https://www.modular.com/blog/ais-compute-fragmentation-what-matrix-multiplication-teaches-us)
        - [2023-04-20ï¼ŒThe world's fastest unified matrix multiplication](https://www.modular.com/blog/the-worlds-fastest-unified-matrix-multiplication)
        - [2023-05-02ï¼ŒA unified, extensible platform to superpower your AI](https://www.modular.com/blog/a-unified-extensible-platform-to-superpower-your-ai)
        - [2023-08-18ï¼ŒHow MojoğŸ”¥ gets a 35,000x speedup over Python â€“ Part 1](https://www.modular.com/blog/how-mojo-gets-a-35-000x-speedup-over-python-part-1)
        - [2023-08-28ï¼ŒHow MojoğŸ”¥ gets a 35,000x speedup over Python â€“ Part 2](https://www.modular.com/blog/how-mojo-gets-a-35-000x-speedup-over-python-part-2)
        - [2023-09-06ï¼ŒMojoğŸ”¥ - A journey to 68,000x speedup over Python - Part 3](https://www.modular.com/blog/mojo-a-journey-to-68-000x-speedup-over-python-part-3)
        - [2024-02-12ï¼ŒMojo vs. Rust: is Mojo ğŸ”¥ faster than Rust ğŸ¦€ ?](https://www.modular.com/blog/mojo-vs-rust-is-mojo-faster-than-rust)
        - [2024-04-10ï¼ŒRow-major vs. column-major matrices: a performance analysis in Mojo and NumPy](https://www.modular.com/blog/row-major-vs-column-major-matrices-a-performance-analysis-in-mojo-and-numpy)




## Videos

  - bilibiliã€Œæ·±åœ³ç‹å“¥çš„ç§‘æŠ€é¢‘é“ã€
    - [2022-06-24ï¼Œã€å¼ å…ˆè½¶ã€‘BLISlabå­¦ä¹ ä¼˜åŒ–çŸ©é˜µä¹˜ã€‚ç¬¬ä¸€è¯¾](https://www.bilibili.com/video/BV1c94y117Uw)
    - [2022-06-24ï¼Œã€å¼ å…ˆè½¶ã€‘BLISlabå­¦ä¹ ä¼˜åŒ–çŸ©é˜µä¹˜ã€‚ç¬¬äºŒè¯¾](https://www.bilibili.com/video/BV1BY411N72y)
    - [2022-06-24ï¼Œã€å¼ å…ˆè½¶ã€‘BLISlabå­¦ä¹ ä¼˜åŒ–çŸ©é˜µä¹˜ã€‚ç¬¬ä¸‰è¯¾](https://www.bilibili.com/video/BV1b94y117BK)
    - [2022-10-19ï¼Œã€å¼ å…ˆè½¶ã€‘BLISlabå­¦ä¹ çŸ©é˜µä¹˜ã€‚ç¬¬å››è¯¾](https://www.bilibili.com/video/BV1oe4y1v7Dm)
    - [2022-09-08ï¼Œã€å¼ å…ˆè½¶ã€‘OpenBLASå¿«é€Ÿå…¥é—¨](https://www.bilibili.com/video/BV1Ze4y1h7GF)
  - bilibiliã€ŒHITsz-OSAã€
    - [2022-07-07ï¼Œç¨ å¯†çŸ©é˜µä¹˜åœ¨å•æ ¸ä¸Šçš„ä¼˜åŒ–](https://www.bilibili.com/video/BV17U4y1D7T8)
  - bilibiliã€ŒæƒåŒã€
    - [2023-07-14ï¼ŒCUDAç¼–ç¨‹åŸºç¡€å…¥é—¨ç³»åˆ—ï¼ˆæŒç»­æ›´æ–°ï¼‰](https://www.bilibili.com/video/BV1sM4y1x7of)




## Interview

  - [Tongkaio/CUDA_Kernel_Samples](https://github.com/Tongkaio/CUDA_Kernel_Samples) <img src="https://img.shields.io/github/stars/Tongkaio/CUDA_Kernel_Samples?style=social"/> : CUDA ç®—å­æ‰‹æ’•ä¸é¢è¯•æŒ‡å—ã€‚

  - å¾®ä¿¡å…¬ä¼—å·ã€Œå¤§æ¨¡å‹ç”Ÿæ€åœˆã€
    - [2024-04-21ï¼Œæ¨ç†éƒ¨ç½²å·¥ç¨‹å¸ˆé¢è¯•é¢˜åº“](https://mp.weixin.qq.com/s/q46vKFPlQhcN7LyZNTRhXA)
  - å¾®ä¿¡å…¬ä¼—å·ã€ŒCverã€
    - [2024-06-01ï¼Œè‹±ä¼Ÿè¾¾ç®—æ³•å²—é¢è¯•ï¼Œé—®çš„è´¼ç»†ï¼](https://mp.weixin.qq.com/s/dwXC572U9u5SAmJPnyjHXA)
  - å¾®ä¿¡å…¬ä¼—å·ã€Œé«˜é€šå†…æ¨ç‹ã€
    - [2023-12-21ï¼Œ[è‹±ä¼Ÿè¾¾å†…æ¨] è‹±*è¾¾é¢è¯•è¿‡ç¨‹å…¨é¢å‰–æ](https://mp.weixin.qq.com/s/GoZKlLfdoGN9ngbe_PzG7w)
    - [2024-04-16ï¼Œä¸€ä»½è‹±ä¼Ÿè¾¾çš„offerï¼Œä¸€å¹´èƒ½åˆ°æ‰‹å¤šå°‘é’±](https://mp.weixin.qq.com/s/dZAG-AXbZkGi9CJQZMhCNA)
  - [çŸ¥ä¹ã€ŒTimåœ¨è·¯ä¸Šâ€‹ã€](https://www.zhihu.com/people/lao-zhang-cao-mei-yuan)
    - [2024-01-18ï¼Œå›½å†…å¤§å‚GPU CUDAé«˜é¢‘é¢è¯•é—®é¢˜æ±‡æ€»ï¼ˆå«éƒ¨åˆ†ç­”æ¡ˆï¼‰](https://zhuanlan.zhihu.com/p/678602674)